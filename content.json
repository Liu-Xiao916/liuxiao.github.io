{"meta":{"title":"XiàoBlog","subtitle":"","description":"The homepage for Xiao","author":"LiuXiao","url":"https://liuxiao916.github.io","root":"/"},"pages":[{"title":"所有分类","date":"2022-05-06T11:56:25.899Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"categories/index.html","permalink":"https://liuxiao916.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2022-05-06T11:56:25.899Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"404.html","permalink":"https://liuxiao916.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2022-05-06T11:56:25.899Z","updated":"2022-05-06T11:56:25.899Z","comments":false,"path":"about/index.html","permalink":"https://liuxiao916.github.io/about/index.html","excerpt":"","text":"Resume Liu Xiao (刘笑) BriefI am a master student majored in Robotics at Harbin Institute of Technology Shenzhen, supervised by Prof. Haoyao Chen. Before this, I got my bacher degree with honor in Harbin Institute of Technology."},{"title":"","date":"2022-05-06T11:56:25.899Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"comments/index.html","permalink":"https://liuxiao916.github.io/comments/index.html","excerpt":"","text":"📬留言版 评论政策 评论请自觉遵守当地法律法规，友善发言。 网友评论仅代表其个人观点，并不表明本站同意其观点或证实其描述"},{"title":"我的朋友们","date":"2022-05-13T12:44:41.816Z","updated":"2022-05-13T12:44:41.816Z","comments":true,"path":"friends/index.html","permalink":"https://liuxiao916.github.io/friends/index.html","excerpt":"这里是笑的朋友们的博客！","text":"这里是笑的朋友们的博客！ 友链申请方式blue前往github新建issue，或者在该页面评论区留言。 提交信息：（名称，头像，链接，网站截图，描述），其中图片请上传图床，提供链接即可。12345title: avatar:url: screenshot: description:"},{"title":"","date":"2022-05-06T11:56:25.903Z","updated":"2022-05-06T11:56:25.903Z","comments":true,"path":"interests/index.html","permalink":"https://liuxiao916.github.io/interests/index.html","excerpt":"","text":"📖笑的书单 开卷有益 佛罗里达 🎥笑的电影 开卷有益 佛罗里达 ⚔️笑的游戏 开卷有益 佛罗里达"},{"title":"","date":"2022-05-06T11:56:25.903Z","updated":"2022-05-06T11:56:25.903Z","comments":false,"path":"photos/index.html","permalink":"https://liuxiao916.github.io/photos/index.html","excerpt":"","text":"📸笑的相册 记录生活中的美好 佛罗里达"},{"title":"","date":"2022-05-06T11:56:25.903Z","updated":"2022-05-06T11:56:25.903Z","comments":true,"path":"photos/florida/index.html","permalink":"https://liuxiao916.github.io/photos/florida/index.html","excerpt":"","text":"佛罗里达 记录生活中的美好"},{"title":"所有标签","date":"2022-05-06T11:56:25.903Z","updated":"2022-05-06T11:56:25.903Z","comments":true,"path":"tags/index.html","permalink":"https://liuxiao916.github.io/tags/index.html","excerpt":"","text":""},{"title":"","date":"2022-05-06T11:56:25.903Z","updated":"2022-05-06T11:56:25.903Z","comments":true,"path":"interests/florida/index.html","permalink":"https://liuxiao916.github.io/interests/florida/index.html","excerpt":"","text":"佛罗里达 记录生活中的美好"}],"posts":[{"title":"LO学习笔记","slug":"Lidar Odometry学习笔记","date":"2022-06-26T02:52:38.000Z","updated":"2022-06-26T03:34:10.998Z","comments":true,"path":"2022/06/26/Lidar Odometry学习笔记/","link":"","permalink":"https://liuxiao916.github.io/2022/06/26/Lidar%20Odometry%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"学习了经典的ALOAM以及Lego-LOAM源代码，这里做一下笔记。","text":"学习了经典的ALOAM以及Lego-LOAM源代码，这里做一下笔记。 Lidar Odometry学习笔记说明： 本文参考了许多博客，参考链接分散在文中，不一一列举。 公式以及排版的渲染有些问题，日后写的时候注意，这一篇可能就不处理了。 LOAM作为最经典的Lidar Odometry算法，是一定要学习一下的。 核心是下面这个程序流程图 Point Cloud RegistrationPoint Cloud Registration中，程序的主要工作是： 根据扫描频率，将每个激光点的scanID信息与距离扫描起始点的时间信息存储到intensity中。 计算每个点的曲度，寻找极大边缘点，次极大边缘点，极小平面点，次极小平面点。然后将他们 publish出去。 1234567891011121314// 计算每一个点的曲率，这里的laserCloud是有序的点云，故可以直接这样计算（论文中说对每条线扫scan计算曲率）// 但是在每条scan的交界处计算得到的曲率是不准确的，这可通过scanStartInd[i]、scanEndInd[i]来选取for (int i = 5; i &lt; cloudSize - 5; i++)&#123; float diffX = laserCloud-&gt;points[i - 5].x + laserCloud-&gt;points[i - 4].x + laserCloud-&gt;points[i - 3].x + laserCloud-&gt;points[i - 2].x + laserCloud-&gt;points[i - 1].x - 10 * laserCloud-&gt;points[i].x + laserCloud-&gt;points[i + 1].x + laserCloud-&gt;points[i + 2].x + laserCloud-&gt;points[i + 3].x + laserCloud-&gt;points[i + 4].x + laserCloud-&gt;points[i + 5].x; float diffY = laserCloud-&gt;points[i - 5].y + laserCloud-&gt;points[i - 4].y + laserCloud-&gt;points[i - 3].y + laserCloud-&gt;points[i - 2].y + laserCloud-&gt;points[i - 1].y - 10 * laserCloud-&gt;points[i].y + laserCloud-&gt;points[i + 1].y + laserCloud-&gt;points[i + 2].y + laserCloud-&gt;points[i + 3].y + laserCloud-&gt;points[i + 4].y + laserCloud-&gt;points[i + 5].y; float diffZ = laserCloud-&gt;points[i - 5].z + laserCloud-&gt;points[i - 4].z + laserCloud-&gt;points[i - 3].z + laserCloud-&gt;points[i - 2].z + laserCloud-&gt;points[i - 1].z - 10 * laserCloud-&gt;points[i].z + laserCloud-&gt;points[i + 1].z + laserCloud-&gt;points[i + 2].z + laserCloud-&gt;points[i + 3].z + laserCloud-&gt;points[i + 4].z + laserCloud-&gt;points[i + 5].z; // 对应论文中的公式（1），但是没有进行除法 cloudCurvature[i] = diffX * diffX + diffY * diffY + diffZ * diffZ; cloudSortInd[i] = i; cloudNeighborPicked[i] = 0; cloudLabel[i] = 0;&#125; Lidar Odometry 使用Ceres进行两次优化，先把所有点投影到当前帧初始的时刻，之后对边缘点（平面点）搜索上一帧最近邻的边或者面，ceres::CostFunction就是两者距离，优化位姿。代价函数定义在lidarFactor.cpp中 最后将当前帧投影到当前帧结束的时刻，将其中的边缘以及平面存储起来下次用。 Lidar Mapping LO程序发布的是当前帧在Odom坐标系下的位姿T_wodom_curr，我们还规定了一个Odom到Map之间的变换关系T_wmap_wodom，因此我们可以得到当前帧在Map之下的位姿T_w_curr = T_wmap_wodom * T_wodom_curr。虽然我们看起来优化的是T_w_curr，实际上的逻辑是．我们通过Ceres得到当前的T_w_curr，之后计算得到T_wmap_wodom，我们实际上实在优化Odom坐标系与Map坐标系之间的转换关系T_wmap_wodom。这样下一帧的位姿T_wodom_curr来的时候，只需要简单的变换就可以得到T_w_curr。 注意在Aloam程序中的地图是只保存4851个Cube的，如果我们的位姿过于靠近地图的某一边，地图会向该方向移动。 123456const int laserCloudWidth = 21; const int laserCloudHeight = 21;const int laserCloudDepth = 11;// the num of cubeconst int laserCloudNum = laserCloudWidth * laserCloudHeight * laserCloudDepth; //4851 我们会在当前位姿的附近取出一个submap，即在当前的cube往各个方向各扩展两个cube，最终能取出来125个cube。然后我们会将submap中的边缘点以及平面点提取出来。 输入的边缘点和平面点进行降采样。之后依然是构造其到submap中对应的直线以及平面的距离的Costfunction，使用Ceres优化两次。这里有一些细微的不同，比如寻找对应直线的时候要找到对应的五个点，然后通过协方差矩阵判断他够不够直。计算协方差矩阵的特征值和特征向量，用于判断这5个点是不是呈线状分布，此为PCA的原理，如果5个点呈线状分布，最大的特征值对应的特征向量就是该线的方向矢量。平面的话则是拟合平面，然后计算点到面的距离。 优化完之后，将当前帧的边缘点以及平面点加入cube中，即加入到地图中。之后会以不同频率发布完整的地图以及submap。 该节点会发布两个位姿，/aft_mapped_to_init_high_frec是在每次接收到新的位姿后，结合当前的T_wmap_wodom，进行一个Odom到Map坐标系的变换后就pub，其频率是和LO相同的。而/aft_mapped_to_init是在每次优化完得到新的T_w_curr后再发送,频率较低. Lego-LOAM Segmentation这个节点主要完成了两个工作 将点云展开到2维图像上，其中每个像素代表深度。 对点云进行分割 首先找到所有的地面点，通过对上下两线相邻点之间俯仰角的计算，如果小于一定阈值就是平面点。 使用BFS对其他点云进行聚类，如果大于30点则认为可以是一类。如果聚类点数小于30大于等于5，此时如果竖直方向超过3条线（比如16线雷达，这个聚类占了三条线）也认为是有效聚类。 聚类数目不够30，但是行数较大，可以认为非地面点的，保存到异常点云（界外点云）（outlier）。 发布： /full_cloud_projected：就是被投影的点云，其index按照投影的坐标排列。每个点的xyz是原始点云的xyz，intensity值是(float)rowIdn + (float)columnIdn / 10000.0得到的，含有坐标信息。 /full_cloud_info：其index按照投影的坐标排列，每个点intensity部分是每个点的深度。 /ground_cloud：所有地面点的点云 /segmented_cloud：所有分割出来的点云以及列数为第五列的地面点 /segmented_cloud_pure：不是地面点以及没有被舍弃的聚类中的点。 /segmented_cloud_info：cloud_info中的信息，包含点云起始角度，结束角度，角度差。第i线的点云起始序列和终止序列。分割点中其是否是地面点，列的index以及深度。 /outlier_cloud：上文所说的异常点云 Feature AssociationIMU数据处理这里先记录一下IMU数据的处理。首先IMU接收回来的加速度数据是相对于IMU坐标系而言的，我们需要将其转换到世界坐标系。IMU接受到的orientation数据是在世界坐标系下旋转的角度。 解得随后我们在IMU坐标系中消去$g_i$的影响。并完成了一个坐标变换(y-&gt;x;z-&gt;y;x-&gt;z)，统一到z轴向前，x轴向左的右手坐标系。交换后 $R_{IW} &#x3D; Ry(yaw)*Rx(pitch)*Rz(roll)$。 12345678910111213141516171819202122232425262728293031void imuHandler(const sensor_msgs::Imu::ConstPtr&amp; imuIn) &#123; double roll, pitch, yaw; tf::Quaternion orientation; tf::quaternionMsgToTF(imuIn-&gt;orientation, orientation); tf::Matrix3x3(orientation).getRPY(roll, pitch, yaw); // 加速度去除重力影响，同时坐标轴进行变换 float accX = imuIn-&gt;linear_acceleration.y - sin(roll) * cos(pitch) * 9.81; float accY = imuIn-&gt;linear_acceleration.z - cos(roll) * cos(pitch) * 9.81; float accZ = imuIn-&gt;linear_acceleration.x + sin(pitch) * 9.81; imuPointerLast = (imuPointerLast + 1) % imuQueLength; imuTime[imuPointerLast] = imuIn-&gt;header.stamp.toSec(); imuRoll[imuPointerLast] = roll; imuPitch[imuPointerLast] = pitch; imuYaw[imuPointerLast] = yaw; imuAccX[imuPointerLast] = accX; imuAccY[imuPointerLast] = accY; imuAccZ[imuPointerLast] = accZ; imuAngularVeloX[imuPointerLast] = imuIn-&gt;angular_velocity.x; imuAngularVeloY[imuPointerLast] = imuIn-&gt;angular_velocity.y; imuAngularVeloZ[imuPointerLast] = imuIn-&gt;angular_velocity.z; AccumulateIMUShiftAndRotation(); &#125; 之后我们需要将其中的信息（位移，速度，角度）转到世界坐标系中。$X_w&#x3D;R_yR_xR_z*X_{IMU}$。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263void AccumulateIMUShiftAndRotation() &#123; float roll = imuRoll[imuPointerLast]; float pitch = imuPitch[imuPointerLast]; float yaw = imuYaw[imuPointerLast]; float accX = imuAccX[imuPointerLast]; float accY = imuAccY[imuPointerLast]; float accZ = imuAccZ[imuPointerLast]; // 先绕Z轴(原x轴)旋转,下方坐标系示意imuHandler()中加速度的坐标轴交换 // z-&gt;Y // ^ // | ^ y-&gt;X // | / // | / // | / // -----&gt; x-&gt;Z // // |cosrz -sinrz 0| // Rz=|sinrz cosrz 0| // |0 0 1| // [x1,y1,z1]^T=Rz*[accX,accY,accZ] // 因为在imuHandler中进行过坐标变换， // 所以下面的roll其实已经对应于新坐标系中(X-Y-Z)的yaw float x1 = cos(roll) * accX - sin(roll) * accY; float y1 = sin(roll) * accX + cos(roll) * accY; float z1 = accZ; // 绕X轴(原y轴)旋转 // [x2,y2,z2]^T=Rx*[x1,y1,z1] // |1 0 0| // Rx=|0 cosrx -sinrx| // |0 sinrx cosrx| float x2 = x1; float y2 = cos(pitch) * y1 - sin(pitch) * z1; float z2 = sin(pitch) * y1 + cos(pitch) * z1; // 最后再绕Y轴(原z轴)旋转 // |cosry 0 sinry| // Ry=|0 1 0| // |-sinry 0 cosry| accX = cos(yaw) * x2 + sin(yaw) * z2; accY = y2; accZ = -sin(yaw) * x2 + cos(yaw) * z2; // 进行位移，速度，角度量的累加 int imuPointerBack = (imuPointerLast + imuQueLength - 1) % imuQueLength; double timeDiff = imuTime[imuPointerLast] - imuTime[imuPointerBack]; if (timeDiff &lt; scanPeriod) &#123; imuShiftX[imuPointerLast] = imuShiftX[imuPointerBack] + imuVeloX[imuPointerBack] * timeDiff + accX * timeDiff * timeDiff / 2; imuShiftY[imuPointerLast] = imuShiftY[imuPointerBack] + imuVeloY[imuPointerBack] * timeDiff + accY * timeDiff * timeDiff / 2; imuShiftZ[imuPointerLast] = imuShiftZ[imuPointerBack] + imuVeloZ[imuPointerBack] * timeDiff + accZ * timeDiff * timeDiff / 2; imuVeloX[imuPointerLast] = imuVeloX[imuPointerBack] + accX * timeDiff; imuVeloY[imuPointerLast] = imuVeloY[imuPointerBack] + accY * timeDiff; imuVeloZ[imuPointerLast] = imuVeloZ[imuPointerBack] + accZ * timeDiff; imuAngularRotationX[imuPointerLast] = imuAngularRotationX[imuPointerBack] + imuAngularVeloX[imuPointerBack] * timeDiff; imuAngularRotationY[imuPointerLast] = imuAngularRotationY[imuPointerBack] + imuAngularVeloY[imuPointerBack] * timeDiff; imuAngularRotationZ[imuPointerLast] = imuAngularRotationZ[imuPointerBack] + imuAngularVeloZ[imuPointerBack] * timeDiff; &#125; &#125; runFeatureAssociation (Pipeline) 判断接收数据的时间戳是否同步； 去除点云畸变。将点云数据进行坐标变换，进行插补等工作； void adjustDistortion()：根据IMU的信息完成了两个任务，首先是将当前时刻世界坐标系下的IMU速度投影到当前帧第一个时刻的IMU的坐标系下，其次是将该帧点云每个点投影到第一个点所在时刻的IMU坐标下，这是去除运动畸变 光滑度计算； 标记不可靠点； 处理两种点，1.舍去处在深度跳跃边缘的点。2.舍去两个点之间突然有较大深度变化的点 特征抽取，然后分别保存到cornerPointsSharp等队列中去； 这个就是根据点的平滑度，然后将角点分为cornerPointsSharp、cornerPointsLessSharp、surfPointsFlat、surfPointsLessFlat。注意平面点只从地面点中选取。 发布cornerPointsSharp等4种类型的点云数据； /laser_cloud_sharp：光滑度最小的两个边缘点 /laser_cloud_less_sharp：边缘点 /laser_cloud_flat：四个最平的平面点 /laser_cloud_less_flat：平面点 预测位姿； 更新变换；https://blog.csdn.net/weixin_44156680/article/details/118302975这一步有些困难，可以分成两部分来看。 先完成平面特征点的匹配，计算每个平面点与对应平面之间的距离这里需要注意的是TransformToStart这个函数。这是利用匀速模型修正当前点云。transformCur是上一时刻与当前时刻的角度以及距离 123456789101112131415161718192021222324tripod1 = laserCloudSurfLast-&gt;points[pointSearchSurfInd1[i]];tripod2 = laserCloudSurfLast-&gt;points[pointSearchSurfInd2[i]];tripod3 = laserCloudSurfLast-&gt;points[pointSearchSurfInd3[i]];float pa = (tripod2.y - tripod1.y) * (tripod3.z - tripod1.z)- (tripod3.y - tripod1.y) * (tripod2.z - tripod1.z);float pb = (tripod2.z - tripod1.z) * (tripod3.x - tripod1.x)- (tripod3.z - tripod1.z) * (tripod2.x - tripod1.x);float pc = (tripod2.x - tripod1.x) * (tripod3.y - tripod1.y)- (tripod3.x - tripod1.x) * (tripod2.y - tripod1.y);float pd = -(pa * tripod1.x + pb * tripod1.y + pc * tripod1.z);float ps = sqrt(pa * pa + pb * pb + pc * pc);pa /= ps;pb /= ps;pc /= ps;pd /= ps;// 距离没有取绝对值// 两个向量的点乘，分母除以ps中已经除掉了，// 加pd原因:pointSel与tripod1构成的线段需要相减float pd2 = pa * pointSel.x + pb * pointSel.y + pc * pointSel.z + pd; 这就是上面那个公式对应的代码，其中pa，pb，pc就是两相量叉乘出来的i，j，k坐标系下的分量，除ps就是除去分母。分子被拆成了两项，前项是pa * pointSel.x + pb * pointSel.y + pc * pointSel.z，后面一向就是减去的pd。最终得到距离pd2。核心思想是混合积代表体积，叉乘代表面积，两个一除就是点到面的距离。 完成边缘特征点的匹配，计算每个边缘特征点与对应边缘之间的距离 接下来和论文中一样，平面点只求解z方向的平移以及picth和roll角度，边缘点求解x、y方向的平移以及yaw角度。https://blog.csdn.net/qq_35102059/article/details/123671942求解的方式是转换为一个最小二乘的优化问题，使用LM法进行迭代求解。(loss为特征点计算出的距离，代码中用B矩阵表示)代码里面实际上是高斯牛顿法，不是LM！ 雅可比矩阵(代码中用A矩阵表示)为：雅科比矩阵实际上是距离(两帧之间的距离，点到面或者点到线)对位姿(6DOF)的求导 $\\frac{\\partial{F}}{\\partial{\\tilde{X}_{k+1}}}$ ：若是线特征则为点到直线方向的单位向量，若是线特征则为点到平面方向的单位向量。 $\\frac{\\partial{\\tilde{X}{k+1}}}{\\partial{T{k+1}}}$ ：可分别对平移和旋转求导。 对于退化问题的分析：https://blog.csdn.net/weixin_44156680/article/details/117999067 积分总变换； 关于AccumulateRotation以及PluginIMURotation作用：博客 这部分主要作用就是将两帧之间的变换累计起来，计算出当前帧到起始点的变换。这里有个细节，你会觉得很多符号写反了，比如transformSum + (-transformCur) =(rx,ry,rz)，实际上就是这样的，因为我们求解的transformCur是点云之间的变换，正好与载具的变换相反的，这点之前我们有推理过。注意这里一些变量的情况， transformSum[]是从一开始到现在的位姿 transformCur[]是两帧之间的位姿 imuPitchStart这帧开始时刻的pitch角 imuPitchLast这帧结束时刻的pitch角度。 PluginIMURotation()就是利用imu修正旋转角度的，因为我们求出来的旋转角度是当前时刻的第一个扫描点的时刻与上一帧最后一个点的时刻的变换角度。但是当前时刻我们需要考虑我们扫描的时间，也就是当前时刻最后一个扫描点与第一个扫描点之间的变换。通过$\\left(R_{\\text {cur }}^{\\text {start }}\\right)^{\\prime}&#x3D;R_{\\text {end }} R_{\\text {start }}^{-1} R_{\\text {cur }}^{\\text {start }}$即可得到。 imuShiftFromStartX是点云最后一个点相对于第一个点由于加减速产生的畸变位移 发布里程计信息及点云信息； 发布： /laser_cloud_surf_last：变换到IMU末端时刻的平面点 /laser_cloud_corner_last：变换到IMU末端时刻的边缘点 /outlier_cloud：上文所说的异常点云 /laser_odom_to_init：从初始到现在的odom信息 Map Optimization特别好的参考博客 run()主体流程 transformAssociateToMap()：根据我个人的理解以及一些资料，这里的作用主要实在优化之前，将坐标从odom坐标系转到map坐标系中，具体的操作是这样的。先介绍一下几个主要的变量。 transformSum[]是当前时刻Odometry模块计算出来的在odom坐标系下的矩阵 transformBefMapped[]是上一次mapping之前的Odometry计算的世界坐标系下的转换矩阵，即上一次mapping时的transformSum[]。 transformAftMapped[]是上一次mapping微调之后的转换矩阵，在map坐标系下。 我们使用$T_{cur}*T_{before}^{-1}$可以得到上一次mapping后，在odom坐标系下车的位姿变换$T_{change}$，之后通过$T_{tobe} &#x3D; T_{change}*T_{after}$ 注意odom坐标系到map坐标系之间存在的修正关系$T_{fix}$，即$T_{after}&#x3D;T_{before}*T_{fix}$ extractSurroundingKeyFrames()： cloudKeyPoses3D中保存的是关键帧的xyz坐标，很聪明的使用点云的形式保持xyz，方便搜索以及降采样。 这个函数是来构造当前帧对应的由关键帧构成的local_map。关键帧的添加在saveKeyFramesAndFactor()函数中。LeGo-LOAM提供了一个比较粗糙的闭环功能，若开启闭环功能，local_map是由一组时间近邻关键帧构成的，即由距离当前帧时间最近的50帧关键帧构成。若关闭闭环功能，local_map是由一组空间近邻关键帧构成的，即由距离当前帧半径50m内的关键帧构成的。不同于LOAM中维护空间范围划分的滑动窗口，LeGo-LOAM这里就比较直接干脆。 满足条件的关键帧的次极大边线点集拼凑起来，构成了local_map中的次极大边线点。（laserCloudCornerFromMapDS）；满足条件的关键帧的次极小平面点集与外点点集拼凑在一块，构成了local_map中的次极小平面点集（laserCloudSurfFromMapDS）。这里是有点问题的！ mapOptmization节点接收到的次极大边线点集与次极小平面点集都是经过去畸变处理的，但是外点点集并没有去畸变，所以次极小平面点集与外点点集的参考坐标不一致，代码中将这俩点云直接拼凑起来。 downsampleCurrentScan()：对来自msg的点云进行降采样。 scan2MapOptimization()：当前帧与local_map进行优化的过程。其中有三个主要的部分 cornerOptimization()：用来计算scan中的边沿点到localmap的边沿点中所匹配的直线的距离。会将距离以及距离的方向向量保存下来 surfOptimization()：找scan中平面点与local_map中的匹配平面，保留平面的参数。 LMOptimization()：实际上是高斯牛顿法，这涉及诈骗啊。 transformUpdate()： saveKeyFramesAndFactor()：添加关键帧，一般与上一关键帧距离相差0.3m则认为是关键帧。同时要进行并进行全局的因子图优化。 correctPoses()：回环检测线程找到闭环并进行闭环帧优化后，在这个线程里更新所有关键帧点云的位姿。 publishTF()：发布结果map optimization之后的位姿。 publishKeyPosesAndFrames()：发布关键帧坐标，还发了当前关键帧附近local_map中的平面点。 回环检测线程 detectLoopClosure()：在关键帧中搜索与当前帧相差超过30s但是距离在7m以内的，认为是可能的回环。将当前帧的边沿点以及平面点合并。将可能的回环帧前后25帧的地图合并构建局部地图。 performLoopClosure()：通过ICP求解当前帧与回环帧之间的位姿变化，然后塞到因子图中优化。 两个gtsam比较有用的材料，个人目前的理解因子图是用于在有回环的时候优化位姿的。 https://programmer.group/gtsam-tutorial-learning-notes.html https://blog.csdn.net/lzy6041/article/details/107658568 可视化线程发布距离当前帧一定范围内的关键帧范围内的点云地图，包括边沿点、平面点以及outliers 发布的Topic /key_pose_origin：关键点的位置XYZ /laser_cloud_surround：就是global_map，当前帧周围一定范围内的地图 /aft_mapped_to_init：优化过后的位姿 /history_cloud：回环检测时，可能回环的那一帧和前后25帧的边沿点以及平面点构成的地图。 /corrected_cloud：回环检测进行ICP后，将当前帧的位姿修正到回环帧处。 /recent_cloud：平面点构成的local_map TransformFusion这里核心的事情，就是将低频的Map Optimization得到的位姿，与高频的雷达里程计发的位姿融合。 transformSum[]是当前时刻Odometry模块计算出来的在odom坐标系下的矩阵 transformBefMapped[]是上一次mapping之前的Odometry计算的世界坐标系下的转换矩阵，即上一次mapping时的transformSum[]。 transformAftMapped[]是上一次mapping微调之后的转换矩阵，在map坐标系下。计算关系还是$T_{map}&#x3D;T_{cur}*T^{-1}{before}*T{after}$，主要内容就是一个odom坐标系到map坐标系的转换。 坐标系梳理 \\camera_init：是打开相机的位置，他与\\map的重合，他们的位姿关系在launch文件中写死 \\aft_mapped：经过Map Optimation后的位姿，明显可以看出频率低了一点。 \\camera：是经过修正后的高频里程计的位姿，其与\\base_link的关系在launch文件中写死。 \\laser_odom：是LO中计算得到的里程计位姿，程序中的transformSum[]。","categories":[{"name":"SLAM","slug":"SLAM","permalink":"https://liuxiao916.github.io/categories/SLAM/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://liuxiao916.github.io/tags/C/"},{"name":"SLAM","slug":"SLAM","permalink":"https://liuxiao916.github.io/tags/SLAM/"}],"author":"LiuXiao"},{"title":"Ceres","slug":"Ceres","date":"2022-03-21T07:19:38.000Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"2022/03/21/Ceres/","link":"","permalink":"https://liuxiao916.github.io/2022/03/21/Ceres/","excerpt":"有必要学习一下Ceres，很多SLAM系统中都用Ceres完成非线性优化任务。","text":"有必要学习一下Ceres，很多SLAM系统中都用Ceres完成非线性优化任务。 简介这是Ceres对一个问题的定义。 其中$\\rho_{i}\\left(\\left|f_{i}\\left(x_{i 1}, \\ldots,x_{i_{k}}\\right)\\right|^{2}\\right)$是ResidualBlock，残差块。 $f_i(*)$ 是CostFunction，代价函数。 $\\rho_{i}$是一个 LossFunction,损失函数。 LossFunction 是一个标量函数，用于减少异常值对非线性最小二乘问题求解的影响。 $[x_{i1},…,x_{i2}]$是ParameterBlock，参数块。 Problem这个是最重要的类。其中的AddResidualBlock是我们最常用的，用于添加残差块。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950ResidualBlockId AddResidualBlock( CostFunction* cost_function, LossFunction* loss_function, const std::vector&lt;double*&gt;&amp; parameter_blocks);// Convenience methods for adding residuals with a small number of// parameters. This is the common case. Instead of specifying the// parameter block arguments as a vector, list them as pointers.ResidualBlockId AddResidualBlock(CostFunction* cost_function, LossFunction* loss_function, double* x0);ResidualBlockId AddResidualBlock(CostFunction* cost_function, LossFunction* loss_function, double* x0, double* x1);ResidualBlockId AddResidualBlock(CostFunction* cost_function, LossFunction* loss_function, double* x0, double* x1, double* x2);ResidualBlockId AddResidualBlock(CostFunction* cost_function, LossFunction* loss_function, double* x0, double* x1, double* x2, double* x3);ResidualBlockId AddResidualBlock(CostFunction* cost_function, LossFunction* loss_function, double* x0, double* x1, double* x2, double* x3, double* x4);ResidualBlockId AddResidualBlock(CostFunction* cost_function, LossFunction* loss_function, double* x0, double* x1, double* x2, double* x3, double* x4, double* x5);ResidualBlockId AddResidualBlock(CostFunction* cost_function, LossFunction* loss_function, double* x0, double* x1, double* x2, double* x3, double* x4, double* x5, double* x6);ResidualBlockId AddResidualBlock(CostFunction* cost_function, LossFunction* loss_function, double* x0, double* x1, double* x2, double* x3, double* x4, double* x5, double* x6, double* x7);ResidualBlockId AddResidualBlock(CostFunction* cost_function, LossFunction* loss_function, double* x0, double* x1, double* x2, double* x3, double* x4, double* x5, double* x6, double* x7, double* x8);ResidualBlockId AddResidualBlock(CostFunction* cost_function, LossFunction* loss_function, double* x0, double* x1, double* x2, double* x3, double* x4, double* x5, double* x6, double* x7, double* x8, double* x9); LocalParameterizationLocalParameterization 接口允许用户定义参数块并与它们所属的流形相关联。它通过定义 Plus () 运算及其雅可比矩阵来实现。 1234567891011121314151617class LocalParameterization &#123; public: virtual ~LocalParameterization() &#123;&#125; // 流型空间中的加法 virtual bool Plus(const double* x, const double* delta, double* x_plus_delta) const = 0; // 计算雅克比矩阵 virtual bool ComputeJacobian(const double* x, double* jacobian) const = 0; // local_matrix = global_matrix * jacobian virtual bool MultiplyByJacobian(const double* x, const int num_rows, const double* global_matrix, double* local_matrix) const; virtual int GlobalSize() const = 0; // 参数块 x 所在的环境空间的维度。 virtual int LocalSize() const = 0; // Δ 所在的切线空间的维度&#125;; 比如我们在计算四元数的时候，参考链接， 四元数表示的是一个SO3，四元数表示的这个东西是一个有三个自由度的东西，然而四元数却有四维也就是四个自由度，这显然是不合理的，所以也就产生了一个单位四元数这么一个东西，单位四元数顾名思义，就是说四元数的四个量的二范数是1。这个其实是一个约束，这个约束就约束了四元数的一个自由度，这样其实四元数就只剩下三个自由度了正好符合一个SO3的维数。 然后在ceres里面，如果使用的是自动求导，然后再结合爬山法，那么每步迭代中都会产生一个四维的delta(迭代的增量，参考LM等算法)，那么根据常规的爬山法，这样就仅仅需要将 原四元数“加上”这个迭代产生的delta就能够得到新的四元数了，这里问题就来了，直接加上以后这个四元数就不在是一个单位四元数了，就没有意义了，如果非得这么用的话就得每次迭代过后都将这个四元数进行一个归一化处理，这显然很麻烦，于是就产生了LocalParameterization。 EigenQuaternionParameterization 1234567891011class CERES_EXPORT EigenQuaternionParameterization : public ceres::LocalParameterization &#123; public: virtual ~EigenQuaternionParameterization() &#123;&#125; virtual bool Plus(const double* x, const double* delta, double* x_plus_delta) const; virtual bool ComputeJacobian(const double* x, double* jacobian) const; virtual int GlobalSize() const &#123; return 4; &#125; virtual int LocalSize() const &#123; return 3; &#125;&#125;; GlobalSize 就是表示他真正的维数是一个4维的，LocalSize是告诉Ceres他表示的东西是一个三维的，然后他定义了一个“Plus”函数，这个函数就是定义的加喽。 除此之外还有 IdentityParameterization() QuaternionParameterization() SubsetParameterization AddParameterBlock123456789101112131415void ProblemImpl::AddParameterBlock(double* values, int size) &#123; InternalAddParameterBlock(values, size);&#125;void ProblemImpl::AddParameterBlock( double* values, int size, LocalParameterization* local_parameterization) &#123; //这行代码和上面的函数是一样的 ParameterBlock* parameter_block = InternalAddParameterBlock(values, size); if (local_parameterization != NULL) &#123; // 之后再对参数块的设置local_parameterization parameter_block-&gt;SetParameterization(local_parameterization); &#125;&#125; 作用都是将参数块添加到problem中。","categories":[{"name":"Programming","slug":"Programming","permalink":"https://liuxiao916.github.io/categories/Programming/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://liuxiao916.github.io/tags/C/"},{"name":"SLAM","slug":"SLAM","permalink":"https://liuxiao916.github.io/tags/SLAM/"}],"author":"LiuXiao"},{"title":"C++知识梳理","slug":"C++知识梳理","date":"2022-03-19T12:27:00.000Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"2022/03/19/C++知识梳理/","link":"","permalink":"https://liuxiao916.github.io/2022/03/19/C++%E7%9F%A5%E8%AF%86%E6%A2%B3%E7%90%86/","excerpt":"","text":"指针与引用123456789101112131415#include &lt;iostream&gt; using namespace std;int main()&#123; int num = 100; int&amp; ref_num = num; ref_num = 110; cout &lt;&lt; &amp;num &lt;&lt;&quot;\\t&quot;&lt;&lt;&amp;ref_num &lt;&lt;endl; int num1 = 100; int* rel_num1 = &amp; num1; *rel_num1 = 110; cout &lt;&lt; &amp;num1 &lt;&lt;&quot;\\t&quot;&lt;&lt;rel_num1&lt;&lt;endl;&#125; 输出为： 我们的结论是： 引用的底层仍然是指针，引用是对指针的封装 获取引用的地址时，编译器内部会进行转换，故引用地址与原变量一样。 函数传参Const12Void TransformToStart(PointType const *const pi, PointType *const po)&#123;&#125;Void TransformToStart(const PointType *const pi, PointType *const po)&#123;&#125; 关键是看const在*的左边还是右边，左边的const修饰PointType表示不能通过指针修改点云，右边的const修饰*表示不能通过该指针指向新的点云。 new 使用new可以在运行阶段分配未命名的内存以存储值，在此情况下只能通过指针来访问内存。 使用new可以释放内存。 12int * ptr_int = new int;delete ptr_int; 初始化参数列表懒人构造法。 12345678class CExample &#123;public: int _a; float _b; //构造函数初始化列表 CExample(int a, float b): _a(a),_b(b) &#123;&#125;&#125;; 可以使用new来初始化一个类，new出来的最后一定要delete。 1CExample* Exp = new CExample(1,2.22) 不new的是在栈内存中，new的是动态分配的堆内存。","categories":[{"name":"Programming","slug":"Programming","permalink":"https://liuxiao916.github.io/categories/Programming/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://liuxiao916.github.io/tags/C/"}],"author":"LiuXiao"},{"title":"Epipolar Geometry","slug":"Epipolar-Geometry","date":"2022-02-20T02:49:00.000Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"2022/02/20/Epipolar-Geometry/","link":"","permalink":"https://liuxiao916.github.io/2022/02/20/Epipolar-Geometry/","excerpt":"这是Stanford CS231A中关于Epipolar Geometry的内容。","text":"这是Stanford CS231A中关于Epipolar Geometry的内容。 对极几何 Epipolar GeometryIntroduction一般我们不能从一张图像中恢复3D世界的信息。因为深度信息的丢失。 图1：这张图让我们觉得人在拖着塔，多角度的照片可以解决这种错觉。 例如，在图 1 中，我们最初可能会误以为这个人正在举起比萨斜塔。 只有仔细观察，我们才能知道情况并非如此，这只是一种基于不同深度投影到像平面上的错觉。 如果我们能够从一个完全不同的角度观看这个场景，这种错觉就会立即消失，我们会立即得到正确的场景。 这些讲义的重点是展示当存在多个相机时掌握几何知识是如何非常有帮助的。 具体来说，我们将首先专注于定义两个视点所涉及的几何，然后介绍这种几何如何帮助进一步了解我们周围的世界。 对极几何 Epipolar Geometry 图2：对极几何的一般形式。灰色区域是极平面（epipolar plane）。 橙色线是基线（baseline），而两条蓝色线是极线（epipolar lines.）。 通常在多视图几何中，多个相机、一个 3D 点以及该点在每个相机图像平面中的投影之间存在有趣的关系。 将相机、3D 中的点和相应的观察相关联的几何称为立体对的对极几何(epipolar geometry of a stereo pair)。 如图 2 所示，通常对极几何包含两个相机观察相同的 3D 点 $P$，其在每个图像平面中的投影分别位于 $p$ 和 $p_0$。 摄像机中心位于 $O_1$ 和 $O_2$，它们之间的线称为基线。 我们称由两个相机中心和 P 定义的平面为极平面。基线与两个图像平面相交的位置称为极点 $e$ 和 $e’$ 。 最后，由极平面和两个图像平面的交点定义的线称为极线。 极线具有在图像平面中的各个极点处与基线相交的特性。 图3：极线以及其对应点的一个例子 图4：当两个图像平面平行时，极点$e$与$e’$在无穷远处。此时极线与图像平面的$u$轴平行。 图 4 显示了一个有趣的对极几何案例，它发生在图像平面彼此平行时。 当图像平面彼此平行时，极点$e$和$e’$将位于无穷远处，因为连接相机中心$O_1$、$O_2$的基线平行于图像平面。 这种情况的另一个重要性质是极线平行于每个图像平面的轴。 这个案例特别有用，将在随后的图像校正部分中更详细地介绍。 然而，在现实世界的情况下，我们没有得到 3D 世界中点 P 的确切位置，但可以确定其在图像平面 p 中的投影。我们还应该能够知道相机的位置、方向和相机矩阵。 我们可以用这些知识做什么？借助相机位置$O_1$、$O_2$和图像点$p$的知识，我们可以定义极平面。有了这个极平面，我们就可以确定极线。根据定义，$P$在第二幅图像中的投影$p’$必须位于第二幅图像的极线上。因此，对极线几何使我们能够在图像对之间创建强约束，而无需了解场景的 3D 信息。 图5：确定基本矩阵和基本矩阵，这有助于跨多个视图映射点和极线。（The setup for determining the essential and fundamental matrices, which help map points and epipolar lines across views.） 我们现在将尝试开发无缝（ seamless）的方法来跨视图映射点和极线。如果我们采用原始对极几何框架（图 5）中给出的配置，那么我们将进一步定义$M$和$M’$为将3D点映射到它们各自的2D图像平面位置的相机投影矩阵。让我们假设世界参考系统与第一个相机相关联，第二个相机首先通过旋转 R 偏移，然后通过平移 T。这指定相机投影矩阵为：$$M &#x3D; K\\begin{bmatrix} I &amp; 0 \\end{bmatrix} \\quad \\ \\ M’&#x3D; K’\\begin{bmatrix} R^T &amp; -R^TT \\end{bmatrix} \\quad$$ 本质矩阵 The Essential Matrix本质矩阵E（Essiential Matrix）：反映空间一点P在不同相机下相机坐标系中的表示之间的关系。 最简单的情况，普通的相机模型，内参相等。所以公式1变成。$$M &#x3D; \\begin{bmatrix} I &amp; 0 \\end{bmatrix} \\quad \\ \\ M’&#x3D; \\begin{bmatrix} R^T &amp; -R^TT \\end{bmatrix} \\quad$$这表明$p’$的位置在第一个相机坐标系中为$Rp’+T$。因此向量$Rp’+T$与$T$都在极平面，之后我们进行叉乘$T\\times(Rp’+T)&#x3D;T\\times(Rp’)$，我们得到一个垂直于极平面的向量。这也意味着$p$在极平面中垂直于$T\\times(Rp’)$，给了我们一个限制是他们的点乘为0：$$p^T[T\\times(Rp’)]&#x3D;0$$我们可以用斜对称矩阵（skew-symmetric）表示叉乘。 $$a\\times b&#x3D; \\begin{bmatrix} 0 &amp; -a_z &amp;a_y\\\\a_z &amp;0 &amp;-a_x\\\\-a_y&amp;a_x&amp;0\\end{bmatrix} \\begin{bmatrix} b_x\\\\b_y\\\\b_z\\end{bmatrix}&#x3D;[a_\\times]b$$ 因此我们可以得到$$p^T[T_\\times]Rp’&#x3D;0$$矩阵$E&#x3D;[T_\\times]R$是本质矩阵，为极线约束创建一个紧凑的表达式：$$p^TEp’&#x3D;0$$其自由度为5（5DOF）。其秩为2且是奇异的。（可以这样理解，旋转以及平移各三个自由度，去掉尺度只剩5个）。 本质矩阵用于计算与$p$和$p’$相关联的极线。比如，$\\ell’&#x3D;E^Tp$得到在相机2的图像平面中的极线。类似的，$\\ell&#x3D;Ep’$得到相机1的图像平面中的极线。另一个有趣的性质是本质矩阵与极点的乘积为0。$E^T &#x3D; Ee’ &#x3D;0$。因为对于任意在相机1的图像平面中的点（除了极点e），对应在相机2的图像中的极线，$l’&#x3D;E^Tx$都包含极点$e’$。因此$e’$满足$e’^T(E^Tx)&#x3D;(e’^TE^T)x&#x3D;0$对于所有的x，所以$Ee’ &#x3D;0$。类似的$E^Te&#x3D;0$。 基础矩阵 The Fundamental Matrix基础矩阵F（Fundamental Matrix）：反映空间一点P在不同相机下图像坐标系中的表示之间的关系。 当我们的相机参数不一致时。$$M &#x3D; K\\begin{bmatrix} I &amp; 0 \\end{bmatrix} \\quad \\ \\ M’&#x3D; K’\\begin{bmatrix} R^T &amp; -R^TT \\end{bmatrix} \\quad$$首先我们定义$p_c&#x3D;K^{-1}p$以及$p_c’ &#x3D; K’^{-1}p’$作为P在相机坐标系下面的投影。根据上一节的推导。$$P_c^T[T_\\times]Rp’_c&#x3D;0$$我们代入可知，得到在像素坐标系下的表示。$$p^TK^{-T}[T_\\times]R*K’^{-1}p’&#x3D;0$$矩阵$F&#x3D;K’^{-T}[T_\\times]RK^{-1}$作为基础矩阵，它的作用类似于上一节中的基本矩阵，但包含有关相机内参矩阵$K,K’$以及相机之间的旋转$R$以及平移$T$的信息。因此，他也有助于计算关联$p$和$p’$的极线，即使当相机内参以及相对变换$R,T$未知的时候。与本质矩阵类似，我们可以通过基础矩阵以及其对应的点计算极线$\\ell’&#x3D;F^Tp$以及$\\ell&#x3D;Fp’$。基础矩阵有七个自由度。（他比单应矩阵少一个自由度，因为skew symmetric matrix的秩为2） 但是基本矩阵有什么用呢？ 与基本矩阵一样，如果我们知道基本矩阵，那么只需知道图像中的一个点就可以为我们提供另一个图像中对应点的简单约束（极线）。 因此，在不知道 P 在 3D 空间中的实际位置或相机的任何外在或内在特征的情况下，我们可以建立任何 $p$与$p’$之间的关系。 八点法我们在不知道相机的外在或内在参数的情况下，可以通过给定同一场景的两幅图像，估计基本矩阵。这种方法叫八点法。八点法假设两个图像之间有一组至少 8 组对应点可用。 图六：两幅图像中对应的八个点。 每对点在对极约束下可得到对应关系$p_i&#x3D;(u_i,v_i,1)$与$p_i’&#x3D;(u_i’,v_i’,1)$，$p_i^TFp’_i&#x3D;0$。我们可以得到： 由于这个约束是一个标量方程，它只约束一个自由度。 因此我们只能需要其中八个约束来确定基本矩阵： 这可以写成$$Wf&#x3D;0$$其中$W$是$N\\times9$的矩阵，f是我们要求的基础矩阵。 实践中，我们常用多于8个点对来求解基础矩阵因为这样可以减少噪声。这个齐次方程组的解可以通过奇异值分解 (SVD) 在最小二乘意义上找到。SVD能得到对基础矩阵的估计$\\hat{F}$，它可能是满秩的。然而，我们知道基础矩阵的秩是2。因此，我们需要找到最好的2阶矩阵来近似$\\hat{F}$。为此，我们解决了以下优化问题： SVD解决这个问题，$\\hat{F}&#x3D;U\\Sigma V^T$，则二阶近似为： 图像校正回想一下，当两个图像彼此平行时，会出现一个有趣的对极几何案例。 让我们首先计算平行图像平面情况下的基本矩阵 E。我们可以假设两个相机具有相同的 K 并且相机之间没有相对旋转 (R &#x3D; I)。在这种情况下，让我们假设只有沿 x 轴的平移，给定 T &#x3D; (Tx, 0, 0)。 得出： 一旦E已知，我们就可以找到与图像平面中的点相关联的极线的方向。 让我们计算与点$p’$相关的极线$\\ell’$的方向 我们可以看出$\\ell$的方向是水平的，$\\ell’$也是。 图 7：图像校正的过程涉及计算两个单应性，我们可以将其应用于一对图像以使它们平行。 图 8：图像校正问题：我们计算两个单应性，我们可以将它们应用于图像平面以使生成的平面平行。","categories":[{"name":"Courese","slug":"Courese","permalink":"https://liuxiao916.github.io/categories/Courese/"}],"tags":[{"name":"SLAM","slug":"SLAM","permalink":"https://liuxiao916.github.io/tags/SLAM/"},{"name":"Computer Visoon","slug":"Computer-Visoon","permalink":"https://liuxiao916.github.io/tags/Computer-Visoon/"}],"author":"LiuXiao"},{"title":"Vision Algorithm for Mobile Robotics课程总结(7~10)节","slug":"Vision-Algorithm-for-Mobile-Robotics课程总结-7-10-节","date":"2022-02-18T13:18:56.000Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"2022/02/18/Vision-Algorithm-for-Mobile-Robotics课程总结-7-10-节/","link":"","permalink":"https://liuxiao916.github.io/2022/02/18/Vision-Algorithm-for-Mobile-Robotics%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93-7-10-%E8%8A%82/","excerpt":"第七讲是说立体视觉以及对极几何，第八讲是本质矩阵以及八点法，第九讲是说RANSAC，第十讲是说SFM以及常见的开源SLAM框架。","text":"第七讲是说立体视觉以及对极几何，第八讲是本质矩阵以及八点法，第九讲是说RANSAC，第十讲是说SFM以及常见的开源SLAM框架。 第七讲 Multiple View Geometry 1Stereo Vision ![](https://xiao-pic.oss-cn-shenzhen.aliyuncs.com/pic/img/截屏2022-01-15 下午3.36.16.png) Depth from StereoGoal: recover the 3D structure by computing the intersection of corresponding rays The Human Binocular System Stereopsys is the principle by which our brain allows us to perceive depth from the left and right images Images project on our retinas up-side-down but our brains lets us perceive them as straight. Radial distortion is also removed. This process is called rectification Triangulation Goal: find an expression of the 3D point coordinates as a function of the 2D image coordinates Assumptions: cameras are calibrated: both intrinsic and extrinsic parameters are known point correspondences are given 为什么大基线，深度误差小？（SLAM十四讲7.6.2） 由公式$Z &#x3D; bf&#x2F;(u_l-u_r)$，可知，比如当物体在10m处， 大基线$b_1f&#x3D;100$，$d &#x3D; u_l-u_r&#x3D;10$，若此时错误测量$d &#x3D; 11$，则Z&#x3D;9.09。若此时错误测量$d &#x3D; 9$，则Z&#x3D;11.11 小基线$b_2f&#x3D;20$，$d &#x3D; u_l-u_r&#x3D;2$，若此时错误测量$d &#x3D; 1$，则Z&#x3D;20。若此时错误测量$d &#x3D; 3$，则Z&#x3D;6.66 为什么大基线，近处盲区大？ 基线$b$大的时候。当距离$Z$小，视差$d$也比较大。因为视差上限固定，故对应的最小距离Z变小。 为什么大基线。搜索困难。 相同距离下，基线越大，视差越大。视差大的话，左右相机图片中对应的位置距离较远，较难搜索。 Triangulation（三角化）知道点在两个相机上的位置，知道位姿变换，求解实际坐标。 Triangulation is the problem of determining the 3D position of a point given a set of corresponding imagelocations and known camera poses We want to intersect the two visual rays corresponding to $p_1$ and $p_2$, but, because of noise and numericalerrors, they won’t meet exactly, so we can only compute an approximation Epipolar GeometryCorrespondence problem The Epipolar Constraint Stereo Rectification 将R，T换成从world坐标系到camera坐标系的转换。 Things to Remember Disparity Triangulation: simplified and general case, linear and non linear approach Choosing the baseline Correspondence problem: epipoles, epipolar lines, epipolar plane Stereo rectification Understanding Check Can you relate Structure from Motion to 3D reconstruction? What’s their difference? 3D重建包含SFM、三维重建包括多种方式，SFM是其中一种。 Can you define disparity in both the simplified and the general case? 简化情况下，相机图像平面是平行的，视差是3D点在两个图像平面上投影位置之间的差。一般情况下图像平面是不平行的，需要先修正。 Can you provide a mathematical expression of depth as a function of the baseline, the disparity and the focal length? $$Z&#x3D;\\frac{bf}{u_l-u_r}$$ Can you apply error propagation to derive an expression for depth uncertainty? How can we improve the uncertainty? 这个误差分析我真的不会。只能看之前我举得例子。Z固定，b越大，d越大，d的小扰动对整体影响越小。 变大基线 高清图像。 Can you analyze the effects of a large&#x2F;small baseline? 大基线，误差比较小 What is the closest depth that a stereo camera can measure? 位于最大视差处 Are you able to show mathematically how to compute the intersection of two lines (linearly and non-linearly)? 三角法吗？问的这是。线性的话有两种 14讲中的方法，构建两个投影点之间的关系，然后叉乘消去一个，得到一个线性方程组。 slide中的方法， 写出两个相机的投影方程，之后叉乘对应的投影点，得到投影点与世界坐标之间的关系，因为相机内参外参已知，使用最小二乘求解即可。 非线性 L-M优化重投影误差 What is the geometric interpretation of the linear and non-linear approaches and what error do they minimize? 线性方法类似优化Ax&#x3D;0，通过SVD求解。 非线性通过L-M优化重投影误差 Are you able to provide a definition of epipole, epipolar line and epipolar plane? 极平面是3D点与两个相机中心构成的平面。极线是极平面与两个相机平面之间的交线。极点是极线与基线的交点 Are you able to draw the epipolar lines for two converging cameras, for a forward motion situation, and for a side-moving camera? OK，主要是把极平面规定出来，接下来都easy。 Are you able to define stereo rectification and to derive mathematically the rectifying homographies? stereo rectification是将两张图像通过构建单应矩阵重投影到极线与扫描线对齐的情况下 How is the disparity map computed? 修正立体图像 寻找对应点 计算视差 How can one establish stereo correspondences with subpixel accuracy? 修正立体图像后，将搜索转换成一维搜索问题，构建窗口。滑动窗口比较两个窗口之间的差距（NCC，用于衡量两张图像间的相关程度）。 需要妥善旋转窗口大小。 大窗口，越光滑，特征少 小窗口，特征多，噪声多。 Describe one or more simple ways to reject outliers in stereo correspondences. 唯一性，同一个点在每个相机平面的投影是唯一的，也就是左侧的相机只能对应右侧的相机中的一个点。 顺序，点在相机平面中的排列顺序是一样的。 梯度。视差是平滑变化的。 Is stereo vision the only way of estimating depth information? If not, are you able to list alternative options? (make link to other lectures) 不是唯一的方法，还有深度学习的方法。 第八讲 Multiple View Geometry 2上一节说的是在相机内外参已知的情况下，通过对极约束搜索对应点的关系，最后通过三角化得到深度信息。这一节要说SFM。 2-view Structure From Motion: Assumptions: none (K, T, and R are unknown). Goal: Recover simultaneously 3D scene structure and camera poses (up to scale) from two images Structure from Motion (SFM) Two variants exist: Calibrated camera(s) ⇒ 𝑲𝟏, 𝑲𝟐 are known Uncalibrated camera(s) ⇒ 𝑲𝟏, 𝑲𝟐 are unknown 先考虑内参已知的情况下 Scale Ambiguity 首先是这么一个情况，我们没有办法通过普通的手段，在没有其他约束的情况下，直接由２Ｄ点恢复出相机位姿，因为会有一个Scale Ambiguity，所以只能恢复五个自由度。但是如果结合极线约束，就可以通过本质矩阵恢复出Ｒ与Ｔ。 这里就可以推导出本质矩阵的表达式了。 Essential Matrix The 8-point algorithm 注意：这里都是归一化平面中的点，本质矩阵都是对归一化平面上的点进行操作的，因为我们已知相机矩阵，所以可以由像素坐标恢复出来归一化平面坐标。 说实话，这里最终的R与T为什么含有内参，我没有搞清楚。所以对于E的分解，已SLAM十四讲为主，推导如下。 本质矩阵的分解https://gutsgwh1997.github.io/2020/05/26/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5%E7%9A%84%E5%88%86%E8%A7%A3/ 再考虑内参不知道的情况下 Fundamental Matrix The 8-point Algorithm for the Fundamental Matrix Error Measures Things to remember SFM from 2 view Calibrated and uncalibrated case Proof of Epipolar Constraint 8-point algorithm and algebraic error Normalized 8-point algorithm Algebraic, directional, Epipolar line distance, Reprojection error Understanding Check What’s the minimum number of correspondences required for calibrated SFM and why? 理论上五个点，因为有4n个已知数解决3n+5个未知数，至少需要五个点的信息。 Are you able to derive the epipolar constraint? 见博客对极几何 Are you able to define the essential matrix? 本质矩阵E（Essiential Matrix）：反映空间一点P在不同相机下相机坐标系中的表示之间的关系。$E&#x3D;[T_\\times]R$ Are you able to derive the 8-point algorithm? Yes How many rotation-translation combinations can the essential matrix be decomposed into? 4 Are you able to provide a geometrical interpretation of the epipolar constraint? 向量P1与P2还有基线必须共平面 Are you able to describe the relation between the essential and the fundamental matrix? 相差了相机内参 Why is it important to normalize the point coordinates in the 8-point algorithm? 因为八点法各参数之间数量级的差异会对最小二乘造成很大的误差。 Describe one or more possible ways to achieve this normalization. 将图像的坐标归一化到-1~1的范围 Are you able to describe the normalized 8-point algorithm? 先利用归一化的坐标计算归一化后的$\\hat{F}$，只有与归一化矩阵一起求解基础矩阵F Are you able to provide quality metrics and their interpretation for the essential and fundamental matrix estimation? Algebraic Error与Directional Error都可以直接衡量，他们是衡量P1、P2以及基线是否在同一平面。 第九讲 Multiple View Geometry 3Robust Structure from Motion Matched points are usually contaminated by outliers (i.e., wrong image matches). Causes of outliers are: Repetitive features changes in view point (including scale) and illumination image noise Occlusions Moving objects blur For reliable and accurate visual odometry, outliers must be removed This is the task of Robust Estimation Expectation Maximization (EM) algorithm EM is a simple method for model fitting in the presence of outliers (very noisy points or wrong data) It can be applied to all sorts of problems where the goal is to estimate the parameters of a model from the data (e.g., camera calibration, Structure from Motion, DLT, PnP, P3P, Homography, etc.) Let’s review EM applied to the line fitting problem Very sensitive to initial condition This is because EM selects the initial condition by minimizing the sum of squared residuals $\\sum r_i^2$. While this is a convex function, the result is strongly influenced by a few large error values (e.g., outliers). Thus, EM converges to the wrong solution if initial condition is far from the true one Alternative options: GNC algorithm RANSAC algorithm Graduated Non-Convexity algorithm (GNC) RANSAC (RAndom SAmple Consensus) RANSAC is the standard method for model fitting in the presence of outliers (very noisy points or wrong data) It is non-deterministic: you get a different result everytime you run it It is not sensitive to the initial condition, and does not get stuck in local maxima It can be applied to all sorts of problems where the goal is to estimate the parameters of a model from the data (e.g., camera calibration, Structure from Motion, DLT, PnP, P3P, Homography, etc.) Let’s review RANSAC for line fitting and see how we can use it to do Structure from Motion In order to implement RANSAC for Structure From Motion (SFM), we need three key ingredients: What’s the model in SFM? The Essential Matrix (for calibrated cameras) or the Fundamental Matrix (for uncalibrated cameras) Alternatively, R and T What’s the minimum number of points to estimate the model? We know that 5 points is the theoretical minimum number of points for calibrated cameras However, if we use the 8-point algorithm, then 8 is the minimum (for both calibrated or uncalibrated cameras) How do we compute the distance of a point from the model? In other words, can we define a distance metric that measures how well a point fits the model? Algebraic error 2. Directional error 3. Epipolar line distance 4. Reprojection error Ransac次数与干扰点比例以及模型需要点的个数相关 一些其他工作 Bundle Adjustment Good to know: Like in the formula, we typically assume the first camera as the world frame, but it’s arbitrary Occasionally, the residual terms are weighted In order to not get stuck in local minima, the initial values of $P_i$, $R$, $𝑇$ should be close to the optimum Can be minimized using Levenberg–Marquardt (more robust than Gauss-Newton to local minima) Can be modified to also optimize the intrinsic parameters What is the key difference with the reprojection error minimization seen in previous lectures (03 and 07)? 第三讲是重投影误差是在相机定位部分，此时三维点坐标不变，不断优化旋转以及平移。 第七讲是三角化问题，此时旋转以及平移是不变的，不断优化三维点坐标。 因为重投影误差较大时，如果计算其平方会比较大，不利于优化，这里采用分段函数来解决这个问题。 Things to remember EM algorithm RANSAC algorithm and its application to SFM 8 vs 5 vs 1 point RANSAC, pros and cons Bundle Adjustmen Understanding Check What are the causes of outliers? Repetitive features changes in view point (including scale) and illumination image noise Occlusions Moving objects blur What effects may outliers have on VO? 估计出来的里程计误差会很大 How does EM work? What are the issues? 通过计算点在模型上的可能性来进行 加权，之后不断优化。 问题是对初值比较敏感 Why do we need RANSAC? RANSAC可以很好的解决outliers，而且准确率很高 What is the theoretical maximum number of combinations to explore? $N*(N-1)&#x2F;2$ After how many iterations can RANSAC be stopped to guarantee a given success probability? $k&#x3D;\\frac{log(1-p)}{log(1-w^2)}$，p是期望准确率，w是内点inlines的比例 What is the trend of RANSAC vs. iterations, vs. the fraction of outliers, vs. the number of points to estimate the model? 外点比例越大，需要迭代次数越多，指数增长。 模型需要点越多，迭代次数越多。 How do we apply RANSAC to the 8-point algorithm, DLT, P3P? 八点法 随机选八个点 计算所有点与这个模型上的距离，决定内点个数 迭代 How can we reduce the number of RANSAC iterations for the SFM problem? (1- and 2-point RANSAC) 通过运动模型的约束，降低需要点的比例 Bundle Adjustment. Mathematical expression and illustration. Tukey and Huber norms. BA是同时优化三维点的坐标以及相机的R与T。 第十讲 Multiple View Geometry 4𝑛-View Structure From Motion Compute initial structure and motion using either: Hierarchical SFM Sequential SFM → Visual Odometry (VO) Refine simultaneously structure and motion through BA Hierarchical SFM Sequential SFM (also called Visual Odometry (VO)) Initialize structure and motion from 2 views (bootstrapping) For each additional view Determine pose (localization) Extend structure, i.e., extract and triangulate new features (mapping) Refine structure and motion through Bundle Adjustment (BA) (optimization) VO Flow Chart Motion Estimation Monocular VO Local Optimization ##### Place Recognition VO vs. Visual SLAM Open Source Algorithms Things to remember Hierarchical SFM VO flowchart Monocular VO Stereo VO Keyframe selection Bundle adjustment vs pose-graph optimization Indirect vs direct methods Direct methods: Dense, semi-dense, and sparse formulations Popular open-source VO algorithms Understanding Check Bundle Adjustment and Pose Graph Optimization. Mathematical expressions and illustrations. Pros and cons. BA是基于重投影误差，优化三维点坐标以及相机位姿。运算量大。 Graph是优化位姿之间的关系。比如T02&#x3D;T01*T12。 Are you able to describe hierarchical and sequential SFM for monocular VO? ​ hierarchical SFM是无序的数据，先实现两两之间的关联，之后再逐层进行三角化，最终拼在一起进行BA。 ​ sequential SFM是有序的数据，计算两帧之间的位移。 What are the building blocks of visual odometry and SLAM? VO的话，特征检测，特征匹配，位姿估计，位姿优化，SLAM再加一个回环检测 What are keyframes? Why do we need them and how can we select them? 关键帧是SLAM系统中，为了提升效率而选取的较为特殊的帧，他们可以保证既有足够的点来计算相机位姿，也有足够的信息来完成三角化。 一般是通过三维点的平均深度除以两帧之间的距离，与一个阈值进行比较。 Are you able to define loop closure detection? Why do we need loops? How can we detect loop closures? (make link to other lectures) 回环检测就是当机器人经过之前走过的地方时，判断自己之前走过，并对当前位姿进行优化，消除累计误差。 因为里程计都有累计误差，因此需要消除。 Are you able to describe the differences between feature-based methods and direct methods? 基于特征的方法是先检测特征点，完成匹配，之后计算位姿。 直接法是计算光度误差，之后直接优化位姿。 Sparse vs semi-dense vs dense. What are their pros and cons? 稀疏的计算更快，消耗资源更少。 半稠密和稠密表现差不多。他们在模糊、少纹理等场合表现更好。 Are you able to provide a list of the most popular open source VO and VSLAM algorithms? 可以","categories":[{"name":"Course","slug":"Course","permalink":"https://liuxiao916.github.io/categories/Course/"}],"tags":[{"name":"SLAM","slug":"SLAM","permalink":"https://liuxiao916.github.io/tags/SLAM/"},{"name":"Robotics","slug":"Robotics","permalink":"https://liuxiao916.github.io/tags/Robotics/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://liuxiao916.github.io/tags/Computer-Vision/"}],"author":"LiuXiao"},{"title":"Vision Algorithm for Mobile Robotics课程总结(5~6)节","slug":"Vision-Algorithm-for-Mobile-Robotics课程总结-5-6-节","date":"2022-02-18T04:15:56.000Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"2022/02/18/Vision-Algorithm-for-Mobile-Robotics课程总结-5-6-节/","link":"","permalink":"https://liuxiao916.github.io/2022/02/18/Vision-Algorithm-for-Mobile-Robotics%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93-5-6-%E8%8A%82/","excerpt":"第五章与第六章是在说特征点的检测。重点是Harris角点检测以及SIFT特征点检测中的细节。","text":"第五章与第六章是在说特征点的检测。重点是Harris角点检测以及SIFT特征点检测中的细节。 Point Feature Detection and Matching – Part 1Filters for Feature detection Goal: reduce amount of data to process in later stages, discard redundancy to preserve only what is useful (leads to lower bandwidth and memory storage) Edge detection (we have seen this already; edges can enable line or shape detection) Template matching Keypoint detection Filters for Template Matching Template Matching will only work if scale, orientation, illumination, and, in general, the appearance of the template (including anything in background) and the object to detect are very similar. Point-feature extraction What features are repeatable and distinctive? 这个定义很有意思 Corner Detection Key observation: in the region around a corner, image gradient has two or more dominant directions Corners are repeatable and distinctive The Harris Corner detector M矩阵很重要，从中就能得到我们需要的信息。可以从特征值与特征向量分析出SSD的变化程度。 Harris workflow Harris Detector: Some Properties Repeatability: How does the Harris detector behave with geometric and photometric changes, i.e. can it re-detect the same corners when the image exhibits changes in Rotation, Scale (zoom), View-point, Illumination ? Summary Filters as templates Correlation as a scalar product Similarity metrics: NCC (ZNCC), SSD (ZSSD), SAD (ZSAD), Census Transform Point feature detection Properties and invariance to transformations Challenges: rotation, scale, view-point, and illumination changes Extraction Moravec Harris and Shi-Tomasi Invariance to rotation, scale, illumination changes Understanding Check Explain what is template matching and how it is implemented? what：模板匹配就是通过现有的模板去与图片进行比较找出图中所匹配的图像 how：模板从图像左上角开始滑动，一直到右下角。之后计算模板与当前区域的相关程度，通过一些衡量方法来确定当前区域与模板是不是相似。NCC越接近1越匹配。SSD与SAD越接近0越匹配。 Explain what are the limitations of template matching? Can you use it to recognize cars? 没有保证尺度（scale）、旋转（orientation）、光照（illumination）不变性。同时要保证模板与待检测物体一模一样。不能用来检测车。车有很多种类型，且车在有移动的时候，可能会尺度、旋转等条件会发生变化。 Illustrate the similarity measures: SSD, SAD, NCC, and Census transform? SSD, SAD, NCC都是按照公式计算模板与待匹配区域之间每个像素的关系，最终得到相应的结果。 census transform是先按照图像块每个像素与中心像素的关系将其编码成一串01的数据，然后计算两串数据之间的汉明距，越小表明越匹配。 What is the intuitive explanation behind SSD and NCC? 将模板与待匹配图像块视为两个向量，向量越接近说明越匹配。 自己问自己的，什么是特征？ 与周围像素有着显著变化的像素块。 Explain what are good features to track? In particular, can you explain what are corners and blobs together with their pros and cons? How is their localization accuracy. 好的特征需要两个性质 重复性（repetition）：同一个特征可在不同的照片中检测出来 独特性（distinctive）：每个特征的描述子不同。不同照片中同一个特征有相同的描述子，同一个图片中每个特征的描述子都与其他特征不同。描述子需要具有几何以及光照不变性。（尺度、旋转、视角、光照） Corners：是两个或多个边的交点。优点是比blob定位更准确，缺点是比blob更难以区分（less distinctive）。 blob：是一种图像类型，首先他不是corner，然后他与周围的像素有显著不同。他的缺点是比corner难以定位（less localization accuracy），优点是更有区分度（more distinctive than corners），有助于场景识别。 Explain the Harris corner detector? In particular: Use the Moravec definition of corner, edge and flat region. 比较窗口滑动（windows shift）之后的像素变化（sum of squared difference, SSD） corner：more distinctive than corners edge：no change along the edge direction no intensity change Show how to get the second moment matrix from the definition of SSD and first order approximation (show that this is a quadratic expression) and what is the intrinsic interpretation of the second moment matrix using a paraboloid and using an ellipse？ $$SSD(\\Delta x,\\Delta y)&#x3D;\\sum_{x,y \\in \\Omega}(I(x,y)-I(x+\\Delta x,y+\\Delta y))^2\\\\&#x3D;\\sum_{x,y \\in \\Omega}(I(x,y)-I(x,y)-I_x(x,y)\\Delta x-I_y(x,y)\\Delta y)^2 \\\\&#x3D;\\sum_{x,y \\in \\Omega}(I_x(x,y)\\Delta x+I_y(x,y)\\Delta y)^2&#x3D;\\sum_{x,y \\in \\Omega}\\begin{bmatrix}\\Delta x&amp;\\Delta y\\end{bmatrix}\\begin{bmatrix}I_x^2 &amp; I_xI_y \\\\ I_xI_y &amp; I_y^2\\end{bmatrix}\\begin{bmatrix}\\Delta x \\\\ \\Delta y\\end{bmatrix}\\\\&#x3D;\\begin{bmatrix}\\Delta x&amp;\\Delta y\\end{bmatrix}\\begin{bmatrix}\\sum I_x^2 &amp; \\sum I_xI_y \\\\ \\sum I_xI_y &amp; \\sum I_y^2\\end{bmatrix}\\begin{bmatrix}\\Delta x \\\\ \\Delta y\\end{bmatrix}$$ M是一个对称阵，可以进行$S &#x3D; Q\\Lambda Q^T$分解，其中Q的列向量是特征向量，代表椭圆的轴的方向，特征值决定轴的长短，特征值越大，轴越短，球径越小，上升越快，梯度越大。 What is the M matrix like for an edge, for a flat region, for an axis-aligned (90-degree) corner and for a non-axis aligned corner? edge只有一个特征值大，flat没有特征值大，corner两个特征值大。通过特征向量区分 axis-aligned (90-degree) corner and for a non-axis aligned corner What do the eigenvalues of M reveal? 特征值越大，轴越短，球径越小，上升越快，梯度越大。说明这个方向上像素值的变化明显。 Can you compare Harris detection with Shi-Tomasi detection? $$Harris:R &#x3D; \\lambda_1 \\lambda_2-k(\\lambda_1+\\lambda_2)^2&#x3D;det(M)-k(trace(M))^2\\\\Shi-Tomasi: R&#x3D;min(\\lambda_1,\\lambda_2)$$ Can you explain whether the Harris detector is invariant to illumination or scale changes? Is it invariant to view point changes? invariant to illumination. 因为像素间的梯度不变 不具有尺度不变性，尺度会改变SSD，即改变梯度。 不一定具有视角不变性。因为视角变了之后，一个corner可能就不是corner了 What is the repeatability of the Harris detector after rescaling by a factor of 2? 18% Point Feature Detection and Matching – Part 2Automatic Scale SelectionScale changesHow can we match image patches corresponding to the same feature but belonging to images taken at different scales? Possible solution: rescale the patch Scale search is time consuming (needs to be done individually for all patches in one image) Complexity is $(NS)^2$ assuming $N$ features per image and $S$ rescalings per feature Solution: automatic scale selection: automatically assign each feature its own “scale” (i.e., size) Automatic Scale Selection一种尺度选择函数，变量是$(x,y,\\sigma)$，横轴是图像块的大小。可以认为在极值点处，两个图像块是相对应的，即在不同尺度下代表相同区域。 Feature descriptors Feature Descriptor Invariance The ideal feature descriptor should be invariant to geometric changes: rotation, scale, view point photometric changes: illumination Most feature methods are designed to be invariant to 2D translation, 2D rotation, Scale Some of them can also handle View-point changes (e.g., SIFT &amp; LIFT work with up to 50 degrees of viewpoint changes) Affine illumination changes HOG descriptor (Histogram of oriented gradients) 这个东西有些厉害，他使用Harris角点检测中使用的M矩阵，用其特征值与特征向量决定的椭圆中的区域。经过归一化后可以形成相同视角的圆形区域。 稍微注意一下这个插值公式，因为I(1,1)离得最远，所以插值时占的比例也越小。 Disadvantage of Patch Descriptors Disadvantage of patch descriptors: If the warp is not estimated accurately, very small errors in rotation, scale, and viewpoint will affect matching score significantly Computationally expensive (need to unwarp every patch) The SIFT blob detector and descriptorSIFT Descriptor SIFT Detector SIFT：Recap SIFT: Scale Invariant Feature Transform An approach to detect and describe regions of interest in an image. SIFT detector &#x3D; DoG detector SIFT features are invariant to 2D rotation, and reasonably invariant to rescaling, viewpoint changes (up to 50 degrees), and illumination It runs in real-time but expensive (10 Hz on an i7 laptop) The expensive steps are the scale detection and descriptor extraction Feature Matching Other corner and blob detectors and descriptors Summary Similarity metrics: NCC (ZNCC), SSD (ZSSD), SAD (ZSAD), Census Transform Point feature detection Properties and invariance to transformations Challenges: rotation, scale, view-point, and illumination changes Extraction Moravec Harris and Shi-Tomasi Rotation invariance Automatic Scale selection Descriptor Intensity patches Canonical representation: how to make them invariant to transformations: rotation, scale, illumination, and viewpoint (affine) Better solution: Histogram of oriented gradients: SIFT descriptor Matching (Z)SSD, SAD, NCC, Hamming distance (last one only for binary descriptors) ratio 1st &#x2F;2nd closest descriptor Depending on the task, you may want to trade off repeatability and robustness for speed: approximated solutions, combinations of efficient detectors and descriptors Fast corner detector: FAST; Keypoint descriptors faster than SIFT: SURF, BRIEF, ORB, BRISK Understanding Check How does automatic scale selection work? 构造了一个和尺度有关的函数，然后代入不同尺度的进行计算，寻找极值点。同一极值点代表不同尺度下的图片的同一位置。 What are the good and the bad properties that a function for automatic scale selection should have or not have? 极值点过于接近，极值点处梯度小。 好的函数需要剧烈的强度变化，便于识别。 How can we implement scale invariant detection efficiently? (show that we can do this by resampling the image vs rescaling the kernel). reshape kernel更好，reshape image计算量更大。 What is a feature descriptor? (patch of intensity value vs histogram of oriented gradients). How do we match descriptors? 描述子可能是多种形式，比如一块intensity信息，或者是梯度方向的直方图（HOG），用于匹配在不同图像中的同一特征点。 通过计算描述子之间的相似度（距离）来完成匹配。 How is the keypoint detection done in SIFT and how does this differ from Harris? Harris主要是通过检测图像x与y方向梯度，完成对corner的检测。SIFT是通过DoF检测不同尺度下的边缘。 How does SIFT achieve orientation invariance? SIFT在计算描述子之前会先计算特征点方向，然后将坐标系旋转到特征点方向计算描述子，保证旋转不变性。 How is the SIFT descriptor built? 在4*4的大网格中，对每个大网格内的16个像素计算梯度方向与强度，然后计算每个大网格的梯度方向直方图，一般为8位，每一位代表45度角。最后得到长度为4*4*8的描述子。 What is the repeatability of the SIFT detector after a rescaling of 2? And for a 50 degrees viewpoint change? 都还不错 Illustrate the 1st to 2nd closest ratio of SIFT detection: what’s the intuitive reasoning behind it? Where does the 0.8 factor come from? 因为过高的维度可能导致向量之间的欧式距离比较接近。这个要求是为了保证距离最小的描述子显著接近于待匹配的描述子。 How does the FAST detector work? What are its pros and cons compared with Harris? 略","categories":[{"name":"Course","slug":"Course","permalink":"https://liuxiao916.github.io/categories/Course/"}],"tags":[{"name":"SLAM","slug":"SLAM","permalink":"https://liuxiao916.github.io/tags/SLAM/"},{"name":"Robotics","slug":"Robotics","permalink":"https://liuxiao916.github.io/tags/Robotics/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://liuxiao916.github.io/tags/Computer-Vision/"}],"author":"LiuXiao"},{"title":"Ubuntu QT程序打包appimage","slug":"Ubuntu-QT程序打包appimage","date":"2022-02-17T12:27:00.000Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"2022/02/17/Ubuntu-QT程序打包appimage/","link":"","permalink":"https://liuxiao916.github.io/2022/02/17/Ubuntu-QT%E7%A8%8B%E5%BA%8F%E6%89%93%E5%8C%85appimage/","excerpt":"在之前用qt做的一个项目需要将开发好的release文件打包成appimage可执行文件，记录一下过程以及遇到的坑。","text":"在之前用qt做的一个项目需要将开发好的release文件打包成appimage可执行文件，记录一下过程以及遇到的坑。 工具下载 linuxdeployqthttps://github.com/probonopd/linuxdeployqt/releases patchelfhttps://nixos.org/releases/patchelf/patchelf-0.9/patchelf-0.9.tar.gz appimagetoolhttps://github.com/AppImage/AppImageKit/releases 工具安装linuxdeployqt1234sudo mv linuxdeployqt-continuous-x86_64.AppImage linuxdeployqtchmod 777 linuxdeployqtsudo mv ./linuxdeployqt /usr/local/binsudo linuxdelpoyqt --version patchelf1234cd patchelf-0.9./configuremakesudo make install appimagetool123sudo mv appimagetool-x86_64.AppImage appimagetoolchmod 777 appimagetoolsudo mv ./appimagetool /usr/local/bin 程序打包创建文件夹文件夹结构如下，其中bin文件夹内的文件为QT在release模式下生成的可执行文件。lib文件夹中为程序的依赖库。 program.destop（没看出来有什么具体作用，但没有.destop文件会报错） 1234567891011[Desktop Entry]Version=1.0Name=appDesignerExec=Terminal=falseType=ApplicationCategories=Development;Icon=programStartupNotify=true Name[en_US]=program.desktop program.png为图标 开始打包^112cd /home/liuxiao/code/Kim_Lab/APP/usr/binlinuxdeployqt program -appimage 这时候会生成一个AppRun，正常应该可以直接双击它运行程序了。 1appimagetool APP (APP是文件夹名)，打包成一个.appimage文件。大功告成。 常见问题依赖so文件的提取1ldd program | awk &#x27;&#123;print $3&#125;&#x27; | xargs -i cp -L &#123;&#125; path 其中program是可执行文件，path是保存路径 qmake路径问题1ERROR: qmake not found on the $PATH 使用qtchooser安装qmake，路径为qmake所在路径。 12qtchooser -install qt5.12 ./Qt5.12.12/5.12.12/gcc_64/bin/qmakeexport QT_SELECT=qt5.12 参考","categories":[{"name":"Tool","slug":"Tool","permalink":"https://liuxiao916.github.io/categories/Tool/"}],"tags":[{"name":"Qt","slug":"Qt","permalink":"https://liuxiao916.github.io/tags/Qt/"}],"author":"LiuXiao"},{"title":"Vision Algorithm for Mobile Robotics课程总结(1~4)节","slug":"Vision-Algorithm-for-Mobile-Robotics课程总结-1-4-节","date":"2022-02-15T12:51:56.000Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"2022/02/15/Vision-Algorithm-for-Mobile-Robotics课程总结-1-4-节/","link":"","permalink":"https://liuxiao916.github.io/2022/02/15/Vision-Algorithm-for-Mobile-Robotics%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93-1-4-%E8%8A%82/","excerpt":"Introduction to Computer Vision and Visual Odometry这门课是ETHZ的muzza教授的，课程设计很棒，上完可以自己手写一个VO，特此记录一下。 第一章是简介，第二章是相机模型与透视投影，第三章是相机标定与PnP问题，第四章是图像的一些滤波以及边缘检测。","text":"Introduction to Computer Vision and Visual Odometry这门课是ETHZ的muzza教授的，课程设计很棒，上完可以自己手写一个VO，特此记录一下。 第一章是简介，第二章是相机模型与透视投影，第三章是相机标定与PnP问题，第四章是图像的一些滤波以及边缘检测。 第一讲 Introduction to Computer Vision and Visual OdometryIntroduction这一节主要介绍了什么是VO。 VO overview定义：VO is the process of incrementally estimating the pose of the vehicle by examining the changes that motion induces on the images of its onboard cameras 假设： Sufficient illumination in the environment Dominance of static scene over moving objects Enough texture to allow apparent motion to be extracted Sufficient scene overlap between consecutive frames VO&amp;VSLAM&amp;SFM SFM is more general than VO and tackles the problem of 3D reconstruction and 6DOF pose estimation from unordered image sets VO focuses on estimating the 6DoF motion of the camera sequentially (as a new frame arrives) and in real time VO fucus on incremental estimation and guarantees local consistency (i.e., estimated trajectory is locally correct, but not globally, i.e. from the start to the end) Visual SLAM &#x3D; visual odometry + loop detection &amp; closure and guarantees global consistency (the estimated trajectory is globally correct, i.e. from the start to the end） VO Flow ChartVO computes the camera path incrementally (pose after pose) Understanding Check Provide a definition of Visual Odometry? Explain the most important differences between VO, VSLAM, and SFM? VO 输入图像是序列 主要是位姿估计 VSLAM 输入图像是序列 VSLAM与VO相比，需要保证全局一致性，即考虑回环 SFM 输入图像是无序的 专注于三维重建 What assumptions does VO rely on? Illustrate the flow chart of VO? 主要分为前端和后端，前端部分用于估计两帧之间的位姿，后端用于优化位姿。 第二讲 Image Formation: perspective projection and camera modelsIntroduction这一讲的重点是相机模型，重点是关注透视模型以及畸变模型。 Image FormationEffects of the Aperture Size（光圈） A large aperture makes the image blurry because a cone of light is let through from each world point Shrinking the aperture makes the image sharper The ideal aperture is a pinhole that only lets through one ray of light from each world point Focal length (焦距) A thin converging lens focuses light onto the film satisfying two properties: Rays passing through the Optical Center are not deviated All rays parallel to the Optical Axis converge at the Focal Point For a given point on the object, there is a specific distance between the lens and the film, at which the object appears in focus in the image Other points project to a blur circle in the image The blur circle has radius: $R&#x3D;L\\delta&#x2F;(2e)$ To capture a sharp image, we must adjust the camera settings such that R remains smaller than the 1 pixel The Pin-hole approximation $Z\\gg f$ and $Z\\gg L$ is known as Pinhole Approximation The relation between the image and object becomes:$x &#x3D; -fX&#x2F;Z$ This is called Perspective Projection（透视投影） Other camera parametersFocus and Depth of Field Depth of Field is the distance between the nearest and farthest objects in a scene that appear acceptably sharp in an image A smaller aperture increases the depth of field but reduces the amount of light into the camera: recall the definition of blur circle (it reduces with aperture) Field of VIew (FOV) FOV is the angular portion of 3D scene seen by the camera As focal length $f$ gets smaller, image becomes more wide angle As focal length $f$ gets larger, image becomes more narrow angle Relation between field of view, $\\theta$, image size, $W$, and focal length, $f$ : $f &#x3D; \\frac{W}{2}[tan\\frac{\\theta}{2}]^{-1}$ Digital cameraDigital Image In a digital camera the film is an array of photodiodes (CCD or CMOS) that convert photons (light energy) into electrons Pixel Intensity with 8 bits ranges between [0,255] Color sensing in digital cameras(了解) Perspective camera modelPerspective Camera For convenience, the image plane is usually represented in front of the lens, 𝑪𝑪, such that the image preserves the same orientation (i.e. not flipped） From World to Pixel coordinates这里有一点很重要，看图片中的绿线，相机坐标系到世界坐标系的转换关系，可以将世界坐标系中点的坐标转换到相机坐标系。 Perspective ProjectionCamera to image Image to pixel Homogeneous Coordinates齐次坐标 World to pixel Normalized image coordinates 归一化平面乘以焦距f就可以转换成成像平面。 Lens distortionRadial Distortion 径向畸变 The standard model of radial distortion is a transformation from the ideal (non-distorted) coordinates (u,v) to the real (distorted) coordinates ($u_d$, $v_d$) For a given non distorted image point (u,v) , the amount of distortion is a nonlinear function of its distance $r$ from the principal point. For most lenses, this simple quadratic model of radial distortion is sufficient: $$\\begin{gathered}{\\left[\\begin{array}{l}u_{d} \\\\v_{d}\\end{array}\\right]&#x3D;\\left(1+k_{1} r^{2}\\right)\\left[\\begin{array}{l}u-u_{0} \\\\v-v_{0}\\end{array}\\right]+\\left[\\begin{array}{l}u_{0} \\\\v_{0}\\end{array}\\right]} \\\\r^{2}&#x3D;\\left(u-u_{0}\\right)^{2}+\\left(v-v_{0}\\right)^{2}\\end{gathered}$$ Tangential Distortion 切向畸变 Radial Distortion: Depending on the amount of distortion (an thus on the camera field of view), higherorder terms can be introduced for the radial distortion Tangential Distortion: if the lens is misaligned (not perfectly parallel to the image sensor), a non-radial(tangential) distortion is introduced Summary This-lens equation From the thin lens to the pinhole camera Perspective Projection Equation Vanishing points and lines Intrinsic and extrinsic parameters (𝑲,𝑹, 𝑻) Homogeneous coordinates Normalized image coordinates Radial distortion Understanding Check Explain what a blur circle is ​ 物体没有正确成像在成像平面上导致的模糊。 Derive the thin lens equation and perform the pinhole approximation Explain how to build an Ames room Derive a relation between the field of view and the focal length 焦距短，FOV大 Proof the perspective projection equation, including lens distortion and world-to-camera projection? Explain normalized image coordinates and their geometric explanation normalized image：虚拟的成像平面，焦距为1。 Define vanishing points and lines 平行线经过透视投影后，相交于灭点 平行平面经过透视投影后，相交于灭面 Prove that parallel lines intersect at vanishing points 第三讲 Camera calibrationCamera calibration Calibration is the process to determine the intrinsic ($K$ plus lens distortion) and extrinsic ($R$, $T$) parameters of a camera. For now, we will neglect the lens distortion and see later how it can be determined. $K,R,T$ can be determined by applying the perspective projection equation to known 3D-2D point correspondences: There are two popular methods: Tsai’s method: uses 3D objects Zhang’s method: uses planar grids Tsai’s Method: Calibration from 3D ObjectsThis method was proposed in 1987 by Tsai and consists of measuring the 3D position of 𝒏 ≥ 𝟔 control points on a 3D calibration target and the 2D coordinates of their projection in the image Direct Linear Transform (DLT) algorithm 直接法The idea of the DLT is to rewrite the perspective projection equation as a homogeneous linear equation (齐次线性方程) and solve it by standard methods. Let’s write the perspective equation for a generic 3D-2D point correspondence: Conversion back from homogeneous coordinates to pixel coordinates leads to: By re-arranging the terms, we obtain For $n$ points, we can stack all these equations into a big matrix: 关于方程的解 Once we have determined M, we can recover the intrinsic and extrinsic parameters by remembering that: $$\\mathrm{M}&#x3D;\\mathrm{K}(\\mathrm{R} | \\mathrm{T})$$ However, notice that we are not enforcing the constraint that $R$ is orthogonal, i.e., $R*R^T&#x3D;I$ To do this, we can use the so-called QR factorization of $M$, which decomposes $M$ into a $R$ (orthogonal), T,and an upper triangular matrix (i.e., $K$) Reprojection Error The reprojection error is the Euclidean distance (in pixels) between an observed image point and the corresponding 3D point reprojected onto the camera frame. The reprojection error gives us a quantitative measure of the accuracy of the calibration (ideally it should be zero). What reprojection error is acceptable? As lower as possible What are the sources of the reprojection error? distortion 离散化导致的，一个像素往往对应现实中的几毫米。 Non-Linear Calibration Refinement Zhang’s method: from planar grids Tsai’s calibration requires that the world’s 3D points are non-coplanar, which is not very practical Today’s camera calibration toolboxes (Matlab, OpenCV) use multiple views of a planar grid (e.g., a checker board) They are based on a method developed in 2000 by Zhang (Microsoft Research) DLTAs in Tsai’s method, we start by writing the perspective projection equation (again, we neglect the radialdistortion). However, in Zhang’s method the points are all coplanar, i.e., $Z_w &#x3D; 0$, and thus we can write: H矩阵就是我们常说的单应矩阵 Conversion back from homogeneous coordinates to pixel coordinates leads to: For n points (from a single view), we can stack all these equations into a big matrix: Homography Camera localization This is the problem of determining the 6DoF pose of a camera (position and orientation) with respect to the world frame from a set of 3D-2D point correspondences. It assumes that the camera is already calibrated The DLT can be used to solve this problem but is suboptimal. We want to study algebraic solutions to the problem. 1 Point: infinite solutions 2 Points: infinite solutions, but bounded 3 Points (non collinear): up to 4 solution It is known that $n$ independent polynomial equations, in $n$ unknowns, can have no more solutions than the product of their respective degrees. Thus, the system can have a maximum of 8 solutions. However, because every term in the system is either a constant or of second degree, for every real positive solution there is a negative solution. Thus, with 3 points, there are at most 4 valid (positive) solutions. 4 Points: Unique solution Robust Estimation in Presence of Outliers All PnP problems (solved by DLT, EPnP, or P3P algorithms) are prone to errors if there are outliers in the set of 3D-2D point correspondences. The RANSAC algorithm (Lecture 08) can be used, in conjunction with the PnP algorithm, to remove the outliers. PnP with RANSAC can be found in OpenCV’s (solvePnPRansac) EPnP vs. DLT Non conventional camera models: fisheye and catadioptric cameras Central vs Non-Central Omnidirectional Cameras 模型还是得看这张图，相比针孔相机模型可以将三维点直接投影到归一化平面，鱼眼相机则多了一个中间过程：先将三维点投影到单位球面，再将单位球面上的点投影到归一化平面上。 Understanding Check Describe the differences between Tsai’s and Zhang’s calibration methods Tsai’s calibration requires that the world’s 3D points are non-coplanar, which is not very practical in Zhang’s method the points are all coplanar Explain and derive the DLT in both Tsai’s and Zhang’s methods? What is the minimum number of point correspondences they require? Tsai：6个点 Zhang：4个非共线的 Describe the general PnP problem and derive the behavior of its solutions? PnP是为了讲解以及2D点，估计相机的3D位姿的任务。一般有DLT，P3P，EPnP的解法 Explain the working principle of the P3P algorithm? P3P主要用的是余弦定理的约束来继续求解，会有4个解，需要用另一个点来找到真正的解。 What is the reprojection error and how is it used for refining the calibration? 现实中的点通过计算的内外参投影到像素坐标系中的误差，可以用作非线性优化的指标。 Define central and non central omnidirectional cameras? What kind of mirrors ensure central projection? 第四讲 Image FilteringDefination： The word filter comes from frequency-domain processing, where “filtering” refers to the process of accepting or rejecting certain frequency components We distinguish between low-pass and high-pass filtering A low-pass filter smooths an image (retains low-frequency components) A high-pass filter retains the contours (also called edges) of an image (high frequency) Low-pass filteringLinear filters Salt and pepper noise: random occurrences of black and white pixels Impulse noise: random occurrences of white pixels Gaussian noise: variations in intensity drawn from a Gaussian distribution Moving average Replaces each pixel with an average of all the values in its neighborhood Assumptions: Expect pixels to be like their neighbors Expect noise process to be independent from pixel to pixel Convolution将输入信号a反过来，在信号b上面滑动得到结果。 Box Filter(average) Gaussian Filter 通过对比，我们可以发现均值滤波相比高斯滤波具有网格化效应。原因是所有的元素不论离中心有多远，权重都是一样的，因此远离的像素信息处理后会融入到中心像素中。所以我们引入高斯滤波会显得更加光滑。 可以看到高斯滤波的滤波核频谱很集中，是一个很典型的低通滤波器。所以其结果图像的频谱也主要集中在频谱图的中心部分。 再来看看box滤波，很明显，它的滤波核的频谱相比高斯滤波核的频谱有更多的高频信息，所以其滤波后的结果也包含更多的高频信息。 这也是为什么其滤波结果不够光滑，有更多条纹状的效应。 Separable Filters Boundary issues Non-linear filters Median Filter Bilateral Filter 双边滤波可以保留边缘 Edge DetectionGoal: to find the boundaries (edges) of objects within images Derivative of Gaussian filter Laplacian of Gaussian filter (LOG) Difference of gaussian filter (DOG)DOG 可以很好的近似LOG，且计算效率更高。 Summary on (linear) smoothing filters Smoothing filter has positive values (also called coefficients) sums to 1 → preserve brightness of constant regions removes “high-frequency” components; “low-pass” filter Derivative filter: has opposite signs used to get high response in regions of high contrast sums to 0 → no response in constant regions highlights “high-frequency” components: “high-pass” filter Canny edge detector Understanding Check Explain the differences between convolution and cross-correlation? 运算方式不同。卷积需要翻转过来。 Explain the differences between a box filter and a Gaussian filter? kernel不一样。box filter的kernel每个元素都一样，Gaussian的kernel则是通过高斯采样获得的。box filter的结果有网格化效应，因为其保留了更多高频的信息，光滑效果不自然。 Explain why one should increase the size of the kernel of a Gaussian filter if 2𝜎 is close to the size of the kernel? 为了能够更好的对高斯核函数进行采样，方差与kernel的大小过于接近的话，采样的结果不能很好的近似高斯分布。 Explain when we would need a median &amp; bilateral filter? 非线性滤波，在去除噪声的同时还能很好的保留边缘信息。 Explain how to handle boundary issues? zero padding wrap around copy edge reflect across edge Explain the working principle of edge detection with a 1D signal? 在一阶导数突变的地方往往是边缘。 Explain how noise does affect this procedure? 噪音会导致一阶导数处处突变，难以分辨一阶导数突变的地方。 Explain the differential property of convolution? Show how to compute the first derivative of an image intensity function along 𝑥 and 𝑦? Use sobel filter Explain why the Laplacian of Gaussian operator is useful? 拉普拉斯算子是二阶导数算子，用于对图像二阶导数进行近似估计。由于拉普拉斯算子对噪声敏感，因此在进行拉普拉斯操作之前先对图像进行高斯平滑滤波处理。 事实上由于卷积操作具有结合律，因此我们先将高斯平滑滤波器与拉普拉斯滤波器进行卷积，然后利用得到的混合滤波器去对图片进行卷积以得到所需的结果。 List the properties of smoothing and derivative filters? Smoothing filter has positive values (also called coefficients) sums to 1 → preserve brightness of constant regions removes “high-frequency” components; “low-pass” filter derivative filter has opposite signs used to get high response in regions of high contrast sums to 0 → no response in constant regions highlights “high-frequency” components: “high-pass” filter Illustrate the Canny edge detection algorithm? Take a grayscale image Convolve the image 𝐼 with 𝑥 and 𝑦 derivatives of Gaussian filter Calculating the direction and strength of edge Thining (Non-maximum suppression): look for local-maxima in the edge strength in the direction of the gradient Linking and Thresholding Explain what non-maxima suppression is and how it is implemented? non-maxima suppression就是搜索局部最大值，寻找边缘梯度方向上梯度强队最大的像素。","categories":[{"name":"Course","slug":"Course","permalink":"https://liuxiao916.github.io/categories/Course/"}],"tags":[{"name":"SLAM","slug":"SLAM","permalink":"https://liuxiao916.github.io/tags/SLAM/"},{"name":"Robotics","slug":"Robotics","permalink":"https://liuxiao916.github.io/tags/Robotics/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://liuxiao916.github.io/tags/Computer-Vision/"}],"author":"LiuXiao"},{"title":"Brave New World!","slug":"Brave-New-World","date":"2022-02-12T03:34:07.000Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"2022/02/12/Brave-New-World/","link":"","permalink":"https://liuxiao916.github.io/2022/02/12/Brave-New-World/","excerpt":"这里是笑的博客，之前的个人主页已经升级成了博客，变成了写字的地方，欢迎常来看看。","text":"这里是笑的博客，之前的个人主页已经升级成了博客，变成了写字的地方，欢迎常来看看。 本站历史 2021-01-12 完成个人主页的搭建，投入使用 2022-02-07 选用Hexo框架，配合volantis主题，开始搭建个人博客 2022-02-12 博客搭建完成，开始进一步完善内容。","categories":[],"tags":[],"author":"LiuXiao"},{"title":"Hexo+volantis搭建博客","slug":"Hexo-volantis搭建博客","date":"2022-02-12T03:16:37.000Z","updated":"2022-05-06T11:56:25.899Z","comments":true,"path":"2022/02/12/Hexo-volantis搭建博客/","link":"","permalink":"https://liuxiao916.github.io/2022/02/12/Hexo-volantis%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/","excerpt":"记录如何使用Hexo+volantis搭建自己的博客","text":"记录如何使用Hexo+volantis搭建自己的博客 Hexo什么是HexoHexo 是一个快速、简单、强大的博客框架。 用 Markdown写好帖子后，Hexo 会在几秒钟内生成带有漂亮主题的静态文件。 环境配置 安装Node.js 安装Hexo 1npm install -g hexo-cli 建站1234mkdir &lt;folder&gt;hexo init &lt;folder&gt;cd &lt;folder&gt;npm install Hexo相关指令请参考Hexo指令 1234hexo clean # 清除缓存文件 (db.json) 和已生成的静态文件 (public)hexo g # 生成景泰文件hexo s # 部署到本地hexo d # 部署到远程 此时使用hexo g &amp;&amp; hexo s，访问http://localhost:4000/ volantis主题综合考虑，决定使用稳定版本的volantis， 修改站点配置文件在 blog&#x2F;_config.yml 文件中找到并修改theme: volantis 下载主题npm i hexo-theme-volantis 此时网站变成这个样子 新建博客123hexo new hellohexo ghexo s 此时在source\\_posts中会新建一个hello.md，我们访问网页可以看到第一篇博文已经添加。 远程部署安装部署插件： 1npm install hexo-deployer-git --save 完成部署 1234deploy: type: git repo: git@github.com:liuxiao916/liuxiao916.github.io.git branch: master 访问liuxiao916.github.io，可以正常访问。 配置Hexo配置参考Hexo配置，鉴于我是新人，仅对Site部分进行修改。 123456789# Sitetitle: XiaoBlogsubtitle: &#x27;&#x27;description: &#x27;The homepage for Xiao&#x27;keywords: Blog, Xiao, LiuXiao, Roboticsauthor: Xiao Liulanguage: zh-CNtimezone: &#x27;Asia/Shanghai&#x27;favicon: https://xiao-pic.oss-cn-shenzhen.aliyuncs.com/pic/img/favicon.png 主题配置参考volantis主题设置 复制配置文件 使用npm i hexo-&gt;theme-volantis方式安装的主题，主题配置文件在blog/node_modules/hexo-theme-volantis/_config.yml。 复制~Blog/node_modules/hexo-theme-volantis/_config.yml为 ~/Blog/_config.volantis.yml，这个文件中的配置信息优先级高于主题文件夹中的配置文件。 修改_config.volantis.yml 关闭自定义鼠标 123custom_css: cursor: enable: false 图标创建网站favicon.ico，色号#0055AA，有趣的色号网站color 关闭自定义右键菜单 123# 自定义右键菜单rightmenu: enable: false 导航栏设置图标库采用font awesome。为了获得更全的图标库，我们安装最新的6.0.0版本 1npm i @fortawesome/fontawesome-free 用/home/liuxiao/Blog/node_modules/@fortawesome替换/home/liuxiao/Blog/node_modules/hexo-theme-volantis/source/libs/@fortawesome。 自定义了一些功能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354navbar: visiable: auto # always, auto logo: # choose [img] or [icon + title] img: https://xiao-pic.oss-cn-shenzhen.aliyuncs.com/pic/img/logo1.png # https://cdn.jsdelivr.net/gh/volantis-x/cdn-org/blog/Logo-NavBar@3x.png icon: # title: #笑 menu: - name: 主页 icon: fas fa-home url: / - name: 文章 icon: fas fa-newspaper rows: - name: 分类 icon: fas fa-folder-open url: categories/ - name: 归档 icon: fas fa-archive url: archives/ - name: 日常 icon: fas fa-icons rows: - name: 相册 icon: fas fa-camera url: photos/ - name: 书单 icon: fas fa-book url: books/ - name: 留言板 icon: fas fa-comments url: comments/ - name: 友链 icon: fas fa-mars url: friends/ - name: 关于我 icon: fas fa-user-circle url: about/ - name: 链接 icon: fas fa-link rows: - name: Github icon: fab fa-github url: https://github.com/liuxiao916 - name: Bilibili icon: fab fa-bilibili url: https://space.bilibili.com/131329867 - name: Zhihu icon: fab fa-zhihu url: https://www.zhihu.com/people/wen-dao-zhu-yao-33 - name: 暗黑模式 # 可自定义 icon: fas fa-moon # 可自定义 toggle: darkmode search: Search... # Search bar placeholder 封面设置设置封面大小为一半，删去功能栏，看起来清爽了不少。 12345678910111213cover: height_scheme: half # full, half layout_scheme: featured # blank (留白), search (搜索), dock (坞), featured (精选), focus (焦点) display: home: true archive: true others: false # can be written in front-matter &#x27;cover: true&#x27; background: https://bing.ioliu.cn/v1/rand?w=1920&amp;h=1200 logo: https://xiao-pic.oss-cn-shenzhen.aliyuncs.com/pic/img/apple-touch-icon.png title: &#x27;Xiao Blog&#x27; subtitle: &#x27;The stars, my destination&#x27; search: A Wonderful Theme for Hexo # search bar placeholder features: 页脚设置 换成更好看的样式。 123456789101112131415161718site_footer: # layout of footer: [aplayer, social, license, info, copyright] layout: [analytics, custom] custom: &#x27;&lt;a style=&quot;padding-right: 1%;&quot; href=&quot;https://hexo.io/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Powered-Hexo-blue&quot;&gt;&lt;/a&gt;&lt;a style=&quot;padding-right: 1%;&quot; href=&quot;https://volantis.js.org/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Theme-Volantis-cyan&quot;&gt;&lt;/a&gt;&lt;a style=&quot;padding-right: 1%;&quot; href=&quot;https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-success&quot;&gt;&lt;/a&gt;&lt;a style=&quot;padding-right: 1%;&quot; href=&quot;https://liuxiao916.github.io&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Copyright-2019--2022%20LiuXiao-orange&quot;&gt;&lt;/a&gt;&#x27; social: - icon: fas fa-rss url: https://github.com/volantis-x/volantis-docs/ # site source source: https://github.com/volantis-x/volantis-docs/ # analytics using leancloud analytics: &gt; &lt;span id=&quot;lc-sv&quot;&gt;本站总访问量为 &lt;span id=&#x27;number&#x27;&gt;&lt;i class=&quot;fas fa-circle-notch fa-spin fa-fw&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/span&gt; 次&lt;/span&gt; &lt;span id=&quot;lc-uv&quot;&gt;访客数为 &lt;span id=&#x27;number&#x27;&gt;&lt;i class=&quot;fas fa-circle-notch fa-spin fa-fw&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/span&gt; 人&lt;/span&gt; # site copyright copyright: &#x27;[Copyright © 2017-2020 XXX](/)&#x27; # You can add your own property here. (Support markdown, for example: br: &#x27;&lt;br&gt;&#x27;) br: &#x27;&lt;br&gt;&#x27; hello: &#x27;[Hello World](/)&#x27; 新建标签与分类界面这里比较麻烦，以标签节目为例。 在hexo根目录的source文件夹下新建一个tags文件夹，然后在tags文件夹里面新建一个index.md文件。 1hexo new page &quot;tags&quot; 编辑index.md文件，其中layout后面的参数对应的是主题文件夹下 layout文件夹下第一级的布局文件/home/liuxiao/Blog/node_modules/hexo-theme-volantis/layout/tag.ejs。 123title: &quot;tags&quot;type: tags layout: &quot;tag&quot; 侧边栏设置打开了站点信息，关闭了赞赏。 设置文章评论 1234567891011121314# giscus# https://giscus.app# https://github.com/laymonage/giscusgiscus: theme: light: https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@master/css/giscus/light.css dark: https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@master/css/giscus/dark.css repo: liuxiao916/liuxiao916.github.io repo-id: MDEwOlJlcG9zaXRvcnkyOTI3OTg1NDg= category: Announcements category-id: DIC_kwDOEXPAVM4CBGFP mapping: &quot;pathname&quot; reactions-enabled: &quot;1&quot; emit-metadata: &quot;0&quot; 烟花效果在node_modules/hexo-theme-volantis/layout/layout.ejs末尾添加 1&lt;script src=&quot;https://cdn.jsdelivr.net/gh/zyoushuo/Blog/hexo/js/mouse_click.js&quot;&gt;&lt;/script&gt; 某些界面的侧边栏设置 这种设置是可以调出来侧边栏设置的，效果如图。 1234567---layout: pageseo_title: 关于bottom_meta: falsecomments: falsesidebar: [blogger]--- 以下设置是不显示侧边栏的设置 1234567---layout: docsseo_title: 关于bottom_meta: falsecomments: falsesidebar: []--- 启用字数统计和阅读时长 配置Leancloud，用于统计访问人数。 相册搭建 使用volantis的标签功能 button 富文本按钮 span note gallery 计划使用github图床配合CDNjsdelivr使用 域名设置现在阿里云万网完成域名的购买，我购买的是liuxiao916.xyz。 点击解析， ping我们的github主页，查看ip。 设置如下的解析。 第二步在我们的github仓库中进行设置，在Settings-&gt;Pages中找到Custom domain，设置成自己的域名。 第三步，在博客的blog/source文件夹下创建一个文件CNAME，输入自己的域名。 再次部署即可。","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://liuxiao916.github.io/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://liuxiao916.github.io/tags/Hexo/"},{"name":"volantis","slug":"volantis","permalink":"https://liuxiao916.github.io/tags/volantis/"}],"author":"LiuXiao"},{"title":"Hello World","slug":"hello-world","date":"2022-02-07T16:00:00.000Z","updated":"2022-02-08T16:00:00.000Z","comments":true,"path":"2022/02/08/hello-world/","link":"","permalink":"https://liuxiao916.github.io/2022/02/08/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[],"author":"LiuXiao"}],"categories":[{"name":"SLAM","slug":"SLAM","permalink":"https://liuxiao916.github.io/categories/SLAM/"},{"name":"Programming","slug":"Programming","permalink":"https://liuxiao916.github.io/categories/Programming/"},{"name":"Courese","slug":"Courese","permalink":"https://liuxiao916.github.io/categories/Courese/"},{"name":"Course","slug":"Course","permalink":"https://liuxiao916.github.io/categories/Course/"},{"name":"Tool","slug":"Tool","permalink":"https://liuxiao916.github.io/categories/Tool/"},{"name":"Hexo","slug":"Hexo","permalink":"https://liuxiao916.github.io/categories/Hexo/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://liuxiao916.github.io/tags/C/"},{"name":"SLAM","slug":"SLAM","permalink":"https://liuxiao916.github.io/tags/SLAM/"},{"name":"Computer Visoon","slug":"Computer-Visoon","permalink":"https://liuxiao916.github.io/tags/Computer-Visoon/"},{"name":"Robotics","slug":"Robotics","permalink":"https://liuxiao916.github.io/tags/Robotics/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://liuxiao916.github.io/tags/Computer-Vision/"},{"name":"Qt","slug":"Qt","permalink":"https://liuxiao916.github.io/tags/Qt/"},{"name":"Hexo","slug":"Hexo","permalink":"https://liuxiao916.github.io/tags/Hexo/"},{"name":"volantis","slug":"volantis","permalink":"https://liuxiao916.github.io/tags/volantis/"}]}